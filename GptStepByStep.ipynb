{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TransformerBlock import TransformerBlock\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "from LayerNorm import LayerNorm\n",
    "from Block import Block\n",
    "from CausalSelfAttention import CausalSelfAttention\n",
    "from MLP import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    context_length: int = 1024\n",
    "    vocab_size: int = 50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self, config:GPTConfig):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.context_length is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.context_length, config.n_embd),\n",
    "                drop=nn.Dropout(config.dropout),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = (\n",
    "            self.lm_head.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for paramName, param in self.named_parameters():\n",
    "            if paramName.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    param, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(param.numel() for param in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x_indices, target_indices=None):\n",
    "        device = x_indices.device\n",
    "        b, t = x_indices.size()\n",
    "        assert (\n",
    "            t <= self.config.context_length\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.context_length}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(x_indices)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "#             print(f'processing {str(block)}')\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if target_indices is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), target_indices.view(-1), ignore_index=-1\n",
    "            ) # cross_entropy() overload used where targets are class indices.\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(\n",
    "                x[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def generate(self, x_token_indices, maxNewTokens:int, topK=None, temperature = 1.0):       \n",
    "#         '''\n",
    "#         temperature: Temperature is a parameter used in natural language processing models to control the \n",
    "#                      degree of randomness or diversity in the generated tokens from an autoregressive language model.\n",
    "#                      The logits are the unnormalized scores that the model assigns to each possible token before applying\n",
    "#                      the softmax function to obtain the probabilities. The temperature parameter is used to scale the logits \n",
    "#                      by dividing them by a positive value. This affects the softmax function and changes the distribution of the probabilities.\n",
    "#                      The impact of dividing the logits by temperature is as follows:\n",
    "#                         * If the temperature is 1, then there is no scaling and the original logits are used. This results in \n",
    "#                         the most likely token being generated according to the model's confidence.\n",
    "#                         * If the temperature is close to 0, then the scaling makes the logits very large and the softmax function \n",
    "#                         becomes very sharp. This results in the model always generating the token with the highest probability,\n",
    "#                         which is equivalent to greedy decoding1.\n",
    "#                         * If the temperature is higher than 1, then the scaling makes the logits smaller and the softmax function \n",
    "#                         becomes flatter. This results in the model generating tokens with lower probabilities more often, which \n",
    "#                         increases the diversity and randomness of the output1.\n",
    "#                     Therefore, temperature can be used to trade-off between quality and diversity of the generated tokens from\n",
    "#                     an autoregressive language model. A lower temperature leads to more coherent but less diverse outputs, \n",
    "#                     while a higher temperature leads to more diverse but less coherent outputs. \n",
    "#                     The optimal value of temperature may depend on the task and the preference of the user.\n",
    "#         '''\n",
    "#         #x_token_indices shape: (batch, sequence_length) \n",
    "#         for nextTokenIndex in range(maxNewTokens):\n",
    "#             conditionedOn_x_token_indices_clipped = (x_token_indices \n",
    "#                                                      if x_token_indices.size(1) < self.config.block_size \n",
    "#                                                      else x_token_indices[:,-self.config.block_size:])\n",
    "\n",
    "\n",
    "#             logits,_ = self(conditionedOn_x_token_indices_clipped) # run forward pass with the conditioned tokens\n",
    "\n",
    "#             logits = logits[:,-1,:] #logits shape (batch,sequence,embedding) => take the last token's full embedding (:) for the batch (:)\n",
    "\n",
    "#             #scale the logits by temperature\n",
    "#             logits = logits / temperature\n",
    "\n",
    "#             if topK is not None:\n",
    "               \n",
    "#                #take the top k embedding features from the logits and set the remaining to -infinity\n",
    "#                #so that they are not connsidered during the softmax.\n",
    "#                topKValues, _ =  torch.topk(logits,k=min(topK,logits.size(-1))) #(batch, sequence, topK) => picks topK embeddings for each sequence in the batch.\n",
    "\n",
    "#                # Across batch (:), for all sequences (:), take the last embedding value (-1) i.e. the minimum value in the topK\n",
    "# #                minValuesInTopKValues shape: (batch, sequence, 1)\n",
    "#                minValuesInTopKValues = topKValues[:,:,[-1]] \n",
    "\n",
    "#                logits[logits < minValuesInTopKValues] = -float(\"Inf\")\n",
    "            \n",
    "#             # apply softmax along the embedding features dimension (-1) to convert \n",
    "#             # logits (shape: batch, sequence, embedding/topK) to normalized probabilities \n",
    "#             probs = torch.nn.functional.softmax(logits,dim=-1)\n",
    "\n",
    "#             next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "#             # append the sampled new token index to the sequence produced so far as an input for next iteration.\n",
    "#             x_token_indices = torch.cat((x_token_indices, next_token_index), dim=1)\n",
    "        \n",
    "#         return x_token_indices\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at context_length\n",
    "            idx_cond = (\n",
    "                idx\n",
    "                if idx.size(1) <= self.config.context_length\n",
    "                else idx[:, -self.config.context_length :]\n",
    "            )\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                # print(f'v.shape: {v.shape}')\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\") # v[:, [-1]] => last i.e. largest element in v \n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "        override_args = override_args or {}  # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == \"dropout\" for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, context_length=1024, bias=True\")\n",
    "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args[\"context_length\"] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args[\"bias\"] = True  # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if \"dropout\" in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args[\"dropout\"] = override_args[\"dropout\"]\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [\n",
    "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
    "        ]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
    "        ]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
    "        ]  # same, just the mask (buffer)\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(\n",
    "            sd_keys\n",
    "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(\n",
    "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "        )\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, lr=learning_rate, betas=betas, **extra_args\n",
    "        )\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    def crop_context_length(self, context_length):\n",
    "        # model surgery to decrease the context length if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (context length 1024)\n",
    "        # but want to use a smaller context length for some smaller, simpler model\n",
    "        assert context_length <= self.config.context_length\n",
    "        self.config.context_length = context_length\n",
    "        self.transformer.wpe.weight = nn.Parameter(\n",
    "            self.transformer.wpe.weight[:context_length]\n",
    "        )\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, \"bias\"):\n",
    "                block.attn.bias = block.attn.bias[:, :, :context_length, :context_length]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptConfig = GPTConfig()\n",
    "customGptModel = GPT(gptConfig)\n",
    "\n",
    "bl = Block(gptConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "hf_gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = hf_gpt2_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "torchinfo.summary(model=hf_gpt2_model,depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(hf_gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(customGptModel)\n",
    "# torchinfo.summary(customGptModel,depth=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare data for fine-turning the pre-trained GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "pdf_fileName = './data/InvestmentBanking.pdf'\n",
    "pdf_fileName = './data/ModernBanking.pdf'\n",
    "out_fileName = pdf_fileName.replace('.pdf','.txt')\n",
    "print(out_fileName)\n",
    "doc = fitz.open(pdf_fileName) # open a document\n",
    "out = open(out_fileName, \"wb\") # create a text output\n",
    "for page in doc: # iterate the document pages\n",
    "\ttext = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "\tout.write(text) # write text of page\n",
    "\tout.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pre-process the data to make it compatible with the model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "input_file_path = './data/ModernBanking.txt'\n",
    "out_folder_path = './data/'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "print(f'Total length of file: {input_file_path} is: {n}')\n",
    "\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(out_folder_path, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(out_folder_path, 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##smaller model config\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = \"out\"\n",
    "eval_interval = 5 #2000\n",
    "log_interval = 1\n",
    "eval_iters = 20 #200\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "init_from = \"gpt2\"  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = \"owt\"\n",
    "wandb_run_name = \"gpt2\"  # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = \"modernBanking\"\n",
    "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
    "batch_size = 6  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_length = 64 #1024\n",
    "# model\n",
    "n_layer = 6 #12\n",
    "n_head = 6 #12\n",
    "n_embd = 128\n",
    "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 2000 #600000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 200 #2000  # how many steps to warm up for\n",
    "lr_decay_iters = 2000 #600000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = (\n",
    "    \"cuda:0\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    ")\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "isWindowsOS = True\n",
    "\n",
    "\n",
    "config_keys = [\n",
    "    k\n",
    "    for k, v in globals().items()\n",
    "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
    "]\n",
    "# exec(open(\"configurator.py\").read())  # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
    "\n",
    "print(str(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = \"out\"\n",
    "eval_interval = 50\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "init_from = \"gpt2\"  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = \"owt\"\n",
    "wandb_run_name = \"gpt2\"  # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = \"modernBanking\"\n",
    "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
    "batch_size = 4  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "context_length = 256\n",
    "# model\n",
    "n_layer = 10\n",
    "n_head = 10\n",
    "n_embd = 256\n",
    "dropout = 0.1  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 600000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 2000  # how many steps to warm up for\n",
    "lr_decay_iters = 600000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = (\n",
    "    \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    ")\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "isWindowsOS = True\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [\n",
    "    k\n",
    "    for k, v in globals().items()\n",
    "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
    "]\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * context_length\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n",
    "\n",
    "print(f'device_type: {device_type}')\n",
    "\n",
    "print(f'dtype: {dtype}')\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]\n",
    "\n",
    "print(f'ptdtype: {ptdtype}')\n",
    "\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "print(f'ctx: {ctx}')\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join(\"./data\", dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "print(f'train data length: {len(train_data)}')\n",
    "print(f'val data length: {len(val_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - context_length, (batch_size,))\n",
    "    x = torch.stack(\n",
    "        [torch.from_numpy((data[i : i + context_length]).astype(np.int64)) for i in ix]\n",
    "    )\n",
    "    y = torch.stack(\n",
    "        [\n",
    "            torch.from_numpy((data[i + 1 : i + 1 + context_length]).astype(np.int64))\n",
    "            for i in ix\n",
    "        ]\n",
    "    )\n",
    "    if device_type == \"cuda\":\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
    "            device, non_blocking=True\n",
    "        )\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    context_length=context_length,\n",
    "    bias=bias,\n",
    "    vocab_size=None,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "# initialize from OpenAI GPT-2 weights\n",
    "override_args = dict(dropout=dropout)\n",
    "model = GPT.from_pretrained(init_from, override_args)\n",
    "# read off the created config params, so we can store them into checkpoint correctly\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"context_length\", \"bias\", \"vocab_size\"]:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "        \n",
    "# crop down the model block size if desired, using model surgery\n",
    "if context_length < model.config.context_length:\n",
    "    model.crop_context_length(context_length)\n",
    "    model_args[\n",
    "        \"context_length\"\n",
    "    ] = context_length  # so that the checkpoint will have the right value\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
    "\n",
    "print(f'Gradscaler enabled: {scaler.is_enabled}')\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
    ")\n",
    "\n",
    "if compile and not isWindowsOS:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "\n",
    "# logging\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    \n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "import time\n",
    "\n",
    "\n",
    "X, Y = get_batch(\"train\")  # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0  # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(\n",
    "            f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        if wandb_log:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"iter\": iter_num,\n",
    "                    \"train/loss\": losses[\"train\"],\n",
    "                    \"val/loss\": losses[\"val\"],\n",
    "                    \"lr\": lr,\n",
    "                    \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "                }\n",
    "            )\n",
    "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    \"model\": raw_model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"model_args\": model_args,\n",
    "                    \"iter_num\": iter_num,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"config\": config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        # if ddp:\n",
    "        #     # in DDP training we only need to sync gradients at the last micro step.\n",
    "        #     # the official way to do this is with model.no_sync() context manager, but\n",
    "        #     # I really dislike that this bloats the code and forces us to repeat code\n",
    "        #     # looking at the source of that context manager, it just toggles this variable\n",
    "        #     model.require_backward_grad_sync = (\n",
    "        #         micro_step == gradient_accumulation_steps - 1\n",
    "        #     )\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = (\n",
    "                loss / gradient_accumulation_steps\n",
    "            )  # scale the loss to account for gradient accumulation\n",
    "        \n",
    "#         print(f'loss in micro step: {micro_step} out of {gradient_accumulation_steps} steps is: {loss}')\n",
    "\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(\"train\")\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        # if local_iter_num >= 5:  # let the training loop settle a bit\n",
    "        #     mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "        #     running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(\n",
    "            f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\"\n",
    "        )\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to try generating some text with our fine-tuned GPT2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "start = \"A Derivative is a contract\"\n",
    "# start = \"Non-banks are\"\n",
    "# start = \"Risk is defined\"\n",
    "start = \"Products offerd by a bank include\"\n",
    "num_samples = 3 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'block_size': 256, 'bias': True, 'vocab_size': 50257, 'dropout': 0.1}\n",
      "{'n_layer': 12, 'n_head': 12, 'n_embd': 768, 'context_length': 256, 'bias': True, 'vocab_size': 50257, 'dropout': 0.1}\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "print(str(checkpoint['model_args']))\n",
    "\n",
    "model_args = {}\n",
    "for k,v in checkpoint['model_args'].items():\n",
    "    if k == 'block_size':\n",
    "        k = 'context_length'\n",
    "    model_args[k] = v\n",
    "    \n",
    "print(model_args)\n",
    "gptconf = GPTConfig(**model_args)\n",
    "\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products offerd by a bank include credit, debit, credit & money\n",
      "management, non-maturity and liquidity products, and life insurance. Customers can download cash\n",
      "from their accounts into accounts and monitor the status of their accounts at any time.\n",
      "Suominen claims three types of payments are offered to banks. A customer is\n",
      "debited at the branch, and deposits are held at the customer for up to 10 years. The customer\n",
      "bears all the risks associated with a loan, and the bank can impose an onerous\n",
      "management regime to meet the customer’s demanding schedule.\n",
      "ž Deposit products: ‘‘private’’ or state-owned – some banks offer some services but others,\n",
      "unlike Finland, offer other ﬁnancial products and services. Customers use a computer to obtain\n",
      "private accounts and deposit funds in real time. In a few countries, such as Germany and Canada,\n",
      "private remote delivery vehicles (RVs) are used to deliver newspapers, sandwiches, bus tickets,\n",
      "or bus tickets to local business areas, without a run on the bank. Some foreign banks\n",
      "also offer banking services to residents and small local ﬁrms in urban areas.\n",
      "\f\n",
      "[ 63 ]\n",
      "D I V E R S I F I C A T I O N\n",
      "O F B A N K I N G A C T I V I T I E S\n",
      "private remote delivery vehicles (trucksharkets) and private internet banking (e-banking). Mobile banking\n",
      "is largely unrestricted among a small number of Grosßanken, the largest private bank in Germany.\n",
      "In the 1990s, Deutsche Telekom partly own a mobile payment service, PayBox.\n",
      "The payment service is offered by PayPal on behalf of the customer, and was\n",
      "launched in Germany in July 2001. Customers with an account at one of the two banks\n",
      "can withdraw the cash from the card without going through the process normally required for an e-\n",
      "purse. The service offered by PayBox involves the customer taking a cheque book (or any other type\n",
      "of cash) and paying for goods and services, through the internet. A website called PayBoxNet is\n",
      " operated by Paycom, and offers customers a direct debit card, credit/debit card, etc.\n",
      "Various services, such as block trading, money market conduct, international coordination and\n",
      "supervision, are offered to bring about the integration of\n",
      "---------------\n",
      "Products offerd by a bank include:\n",
      "ž Commercial banking;\n",
      "ž Investment banking;\n",
      "ž Corporate ﬁnance;\n",
      "ž Real estate services.\n",
      "The FSA is also obliged to be cost effective, to include:\n",
      "ž A reduction in the duplication of FSA supervision and control procedures, to be\n",
      "sophisticated across the service areas. For example, the FSA must be more cost effective when employing\n",
      "risk management techniques such as scenario analysis and stress tests.\n",
      "ž After a merger or acquisition, the bank is required to report (annually) any changes\n",
      "in performance that affect earnings, and the FSA may require banks to subtract any equity losses\n",
      "pre-merger from the capital base post-merger.\n",
      "ž In addition to the Basel risk assets ratios and pillar 3 risk assets ratios, the FSA also publishes a\n",
      "resource sheet with more detail on how the bank’s internal model should be run.\n",
      "Generally, the more sophisticated the bank, the higher the capital set aside. For example,\n",
      "SMEs are likely to have sophisticated risk management systems in place if the size of their business\n",
      "is small. Large universal banks may expect to incur large capital requirements, but it will\n",
      "be difﬁcult for supervisors to compartmentalise this type of risk. Large universal banks may want\n",
      "to reduce the amount of capital they are required to set aside, but will lack the\n",
      "information necessary to fully assess the risk arising from such a diversiﬁed portfolio.\n",
      "Regulators will want to allow for diversiﬁcation gains within a ﬁnancial conglomerate, and\n",
      "additional risk weightings based on the group’s risk proﬁle. For example, a merger has the potential\n",
      "to increase credit risk for the ‘‘other’’ group because many of the same group of banks pay the same\n",
      "premium for the same services, so the aggregate capital charge will be lower. This is an example\n",
      "of a ‘‘too big to fail’’ policy.\n",
      "Mergers and Acquisitions: another way of dealing with market risk, it is when a ﬁrm\n",
      "engaged in mergers/acquisitions assumes some of the market risk, usually through the\n",
      "sale of its own shares. The underlying asset is the equity, usually bought and sold on an\n",
      "established market. The acquired banks are more d\n",
      "---------------\n",
      "Products offerd by a bank include other ﬁnancial services not available elsewhere. For\n",
      "example, they can supply credit services to corporate customers and pension funds.\n",
      "Provided a bank can act as intermediary at the lowest possible cost, there will\n",
      "be a movement of deposits out of the sector. No single model of liquidity risk applies to all banks,\n",
      "so it is difﬁcult to apply the same ideas to all different types of banking activities. Nonetheless,\n",
      "banks do share some common features. The key difference between liquidity risk and money market\n",
      "risk is that liabilities for liabilities underwriting are more closely correlated with each other,\n",
      "resulting in a more liquid ﬁnancial system.\n",
      "Suppose there are two money markets, ﬁnancial markets and interest rate markets. The\n",
      "principal agent will want to know the interest rate at which the investor will be able to withdraw\n",
      "his/her savings. The prudential regulator may be able to impose a liquidity ratio of 8% on banks\n",
      "with a market clearing price in excess of 95%, or an equivalent in some cases, may allow for the\n",
      "option of a 100% coverage. A bank with a market clearing price in excess of 95% would be\n",
      "subject to the same regulation as one with a market clearing price of less than 95%. A bank\n",
      "with market clearing prices in excess of 80% will be required to pay a quarterly premium of 0.08%\n",
      "on deposits, and pay a quarterly premium of 0.88% on loans. Subsequently, an investor with a\n",
      "different retail interest rate exposure will be required to pay the same quarterly premium, even if the deposit\n",
      "is for six months. The depositor is unlikely to be able to withdraw the deposit in 6 months’ time,\n",
      "but the loan is still outstanding and the investor is covered for six months. Likewise for\n",
      "a mortgage related to collateral, covered by a collateralised loan, that is, the liability of the\n",
      "mortgagees. However, for mortgages with collateral, covering the deposit insurance premium is not straightforward.\n",
      "For example, in the UK, the generic term for collateralised loan (CD) is ‘‘guarantee’’ – that is,\n",
      "the government guarantees, in the form of guarantees, loans to the creditors and, where applicable,\n",
      "liquidity support to solvent ﬁrms in the event of bankruptcy. The IMF has inﬂated on\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(globals().items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
