{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TransformerBlock import TransformerBlock\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "from LayerNorm import LayerNorm\n",
    "from Block import Block\n",
    "from CausalSelfAttention import CausalSelfAttention\n",
    "from MLP import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GPT(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(\n",
    "            dict(\n",
    "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "                drop=nn.Dropout(config.dropout),\n",
    "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f=LayerNorm(config.n_embd, bias=config.bias),\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = (\n",
    "            self.lm_head.weight\n",
    "        )  # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith(\"c_proj.weight\"):\n",
    "                torch.nn.init.normal_(\n",
    "                    p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer)\n",
    "                )\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert (\n",
    "            t <= self.config.block_size\n",
    "        ), f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)  # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "#             print(f'processing {str(block)}')\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1\n",
    "            )\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(\n",
    "                x[:, [-1], :]\n",
    "            )  # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def generate(self, x_token_indices, maxNewTokens:int, topK=None, temperature = 1.0):       \n",
    "#         '''\n",
    "#         temperature: Temperature is a parameter used in natural language processing models to control the \n",
    "#                      degree of randomness or diversity in the generated tokens from an autoregressive language model.\n",
    "#                      The logits are the unnormalized scores that the model assigns to each possible token before applying\n",
    "#                      the softmax function to obtain the probabilities. The temperature parameter is used to scale the logits \n",
    "#                      by dividing them by a positive value. This affects the softmax function and changes the distribution of the probabilities.\n",
    "#                      The impact of dividing the logits by temperature is as follows:\n",
    "#                         * If the temperature is 1, then there is no scaling and the original logits are used. This results in \n",
    "#                         the most likely token being generated according to the model's confidence.\n",
    "#                         * If the temperature is close to 0, then the scaling makes the logits very large and the softmax function \n",
    "#                         becomes very sharp. This results in the model always generating the token with the highest probability,\n",
    "#                         which is equivalent to greedy decoding1.\n",
    "#                         * If the temperature is higher than 1, then the scaling makes the logits smaller and the softmax function \n",
    "#                         becomes flatter. This results in the model generating tokens with lower probabilities more often, which \n",
    "#                         increases the diversity and randomness of the output1.\n",
    "#                     Therefore, temperature can be used to trade-off between quality and diversity of the generated tokens from\n",
    "#                     an autoregressive language model. A lower temperature leads to more coherent but less diverse outputs, \n",
    "#                     while a higher temperature leads to more diverse but less coherent outputs. \n",
    "#                     The optimal value of temperature may depend on the task and the preference of the user.\n",
    "#         '''\n",
    "#         #x_token_indices shape: (batch, sequence_length) \n",
    "#         for nextTokenIndex in range(maxNewTokens):\n",
    "#             conditionedOn_x_token_indices_clipped = (x_token_indices \n",
    "#                                                      if x_token_indices.size(1) < self.config.block_size \n",
    "#                                                      else x_token_indices[:,-self.config.block_size:])\n",
    "\n",
    "\n",
    "#             logits,_ = self(conditionedOn_x_token_indices_clipped) # run forward pass with the conditioned tokens\n",
    "\n",
    "#             logits = logits[:,-1,:] #logits shape (batch,sequence,embedding) => take the last token's full embedding (:) for the batch (:)\n",
    "\n",
    "#             #scale the logits by temperature\n",
    "#             logits = logits / temperature\n",
    "\n",
    "#             if topK is not None:\n",
    "               \n",
    "#                #take the top k embedding features from the logits and set the remaining to -infinity\n",
    "#                #so that they are not connsidered during the softmax.\n",
    "#                topKValues, _ =  torch.topk(logits,k=min(topK,logits.size(-1))) #(batch, sequence, topK) => picks topK embeddings for each sequence in the batch.\n",
    "\n",
    "#                # Across batch (:), for all sequences (:), take the last embedding value (-1) i.e. the minimum value in the topK\n",
    "# #                minValuesInTopKValues shape: (batch, sequence, 1)\n",
    "#                minValuesInTopKValues = topKValues[:,:,[-1]] \n",
    "\n",
    "#                logits[logits < minValuesInTopKValues] = -float(\"Inf\")\n",
    "            \n",
    "#             # apply softmax along the embedding features dimension (-1) to convert \n",
    "#             # logits (shape: batch, sequence, embedding/topK) to normalized probabilities \n",
    "#             probs = torch.nn.functional.softmax(logits,dim=-1)\n",
    "\n",
    "#             next_token_index = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "#             # append the sampled new token index to the sequence produced so far as an input for next iteration.\n",
    "#             x_token_indices = torch.cat((x_token_indices, next_token_index), dim=1)\n",
    "        \n",
    "#         return x_token_indices\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = (\n",
    "                idx\n",
    "                if idx.size(1) <= self.config.block_size\n",
    "                else idx[:, -self.config.block_size :]\n",
    "            )\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
    "        override_args = override_args or {}  # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == \"dropout\" for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
    "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
    "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
    "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
    "        config_args[\"bias\"] = True  # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if \"dropout\" in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args[\"dropout\"] = override_args[\"dropout\"]\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [\n",
    "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
    "        ]  # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
    "        ]  # ignore these, just a buffer\n",
    "        sd_keys_hf = [\n",
    "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
    "        ]  # same, just the mask (buffer)\n",
    "        transposed = [\n",
    "            \"attn.c_attn.weight\",\n",
    "            \"attn.c_proj.weight\",\n",
    "            \"mlp.c_fc.weight\",\n",
    "            \"mlp.c_proj.weight\",\n",
    "        ]\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(\n",
    "            sd_keys\n",
    "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": nodecay_params, \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(\n",
    "            f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\"\n",
    "        )\n",
    "        print(\n",
    "            f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\"\n",
    "        )\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = \"fused\" in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optim_groups, lr=learning_rate, betas=betas, **extra_args\n",
    "        )\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "    \n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(\n",
    "            self.transformer.wpe.weight[:block_size]\n",
    "        )\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, \"bias\"):\n",
    "                block.attn.bias = block.attn.bias[:, :, :block_size, :block_size]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gptConfig = GPTConfig()\n",
    "customGptModel = GPT(gptConfig)\n",
    "\n",
    "bl = Block(gptConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "hf_gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = hf_gpt2_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "print(inputs[\"input_ids\"].shape)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "\n",
    "torchinfo.summary(model=hf_gpt2_model,depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params+=params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "\n",
    "count_parameters(hf_gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_parameters(customGptModel)\n",
    "# torchinfo.summary(customGptModel,depth=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare data for fine-turning the pre-trained GPT2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "pdf_fileName = './data/InvestmentBanking.pdf'\n",
    "pdf_fileName = './data/ModernBanking.pdf'\n",
    "out_fileName = pdf_fileName.replace('.pdf','.txt')\n",
    "print(out_fileName)\n",
    "doc = fitz.open(pdf_fileName) # open a document\n",
    "out = open(out_fileName, \"wb\") # create a text output\n",
    "for page in doc: # iterate the document pages\n",
    "\ttext = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n",
    "\tout.write(text) # write text of page\n",
    "\tout.write(bytes((12,))) # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pre-process the data to make it compatible with the model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "input_file_path = './data/ModernBanking.txt'\n",
    "out_folder_path = './data/'\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "n = len(data)\n",
    "print(f'Total length of file: {input_file_path} is: {n}')\n",
    "\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode with tiktoken gpt2 bpe\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "train_ids = enc.encode_ordinary(train_data)\n",
    "val_ids = enc.encode_ordinary(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(out_folder_path, 'train.bin'))\n",
    "val_ids.tofile(os.path.join(out_folder_path, 'val.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##smaller model config\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = \"out\"\n",
    "eval_interval = 5 #2000\n",
    "log_interval = 1\n",
    "eval_iters = 20 #200\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "init_from = \"gpt2\"  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = \"owt\"\n",
    "wandb_run_name = \"gpt2\"  # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = \"modernBanking\"\n",
    "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
    "batch_size = 6  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 64 #1024\n",
    "# model\n",
    "n_layer = 6 #12\n",
    "n_head = 6 #12\n",
    "n_embd = 128\n",
    "dropout = 0.0  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 2000 #600000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 200 #2000  # how many steps to warm up for\n",
    "lr_decay_iters = 2000 #600000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = (\n",
    "    \"cuda:0\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    ")\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "isWindowsOS = True\n",
    "\n",
    "\n",
    "config_keys = [\n",
    "    k\n",
    "    for k, v in globals().items()\n",
    "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
    "]\n",
    "# exec(open(\"configurator.py\").read())  # overrides from command line or config file\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging\n",
    "\n",
    "print(str(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# default config values designed to train a gpt2 (124M) on OpenWebText\n",
    "# I/O\n",
    "out_dir = \"out\"\n",
    "eval_interval = 50\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False  # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True  # if True, always save a checkpoint after each eval\n",
    "init_from = \"gpt2\"  # 'scratch' or 'resume' or 'gpt2*'\n",
    "# wandb logging\n",
    "wandb_log = False  # disabled by default\n",
    "wandb_project = \"owt\"\n",
    "wandb_run_name = \"gpt2\"  # 'run' + str(time.time())\n",
    "# data\n",
    "dataset = \"modernBanking\"\n",
    "gradient_accumulation_steps = 5 * 8  # used to simulate larger batch sizes\n",
    "batch_size = 4  # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 256\n",
    "# model\n",
    "n_layer = 10\n",
    "n_head = 10\n",
    "n_embd = 256\n",
    "dropout = 0.1  # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False  # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4  # max learning rate\n",
    "max_iters = 600000  # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0  # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True  # whether to decay the learning rate\n",
    "warmup_iters = 2000  # how many steps to warm up for\n",
    "lr_decay_iters = 600000  # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5  # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = \"nccl\"  # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = (\n",
    "    \"cuda\"  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    ")\n",
    "dtype = (\n",
    "    \"bfloat16\"\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else \"float16\"\n",
    ")  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "compile = True  # use PyTorch 2.0 to compile the model to be faster\n",
    "\n",
    "isWindowsOS = True\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [\n",
    "    k\n",
    "    for k, v in globals().items()\n",
    "    if not k.startswith(\"_\") and isinstance(v, (int, float, bool, str))\n",
    "]\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys}  # will be useful for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 40,960\n",
      "device_type: cuda\n",
      "dtype: float16\n",
      "ptdtype: torch.float16\n",
      "ctx: <torch.amp.autocast_mode.autocast object at 0x0000020896851A20>\n",
      "train data length: 506290\n",
      "val data length: 79438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True  # allow tf32 on cudnn\n",
    "device_type = \"cuda\" if \"cuda\" in device else \"cpu\"  # for later use in torch.autocast\n",
    "\n",
    "print(f'device_type: {device_type}')\n",
    "\n",
    "print(f'dtype: {dtype}')\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {\n",
    "    \"float32\": torch.float32,\n",
    "    \"bfloat16\": torch.bfloat16,\n",
    "    \"float16\": torch.float16,\n",
    "}[dtype]\n",
    "\n",
    "print(f'ptdtype: {ptdtype}')\n",
    "\n",
    "ctx = (\n",
    "    nullcontext()\n",
    "    if device_type == \"cpu\"\n",
    "    else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    ")\n",
    "\n",
    "print(f'ctx: {ctx}')\n",
    "\n",
    "# poor man's data loader\n",
    "data_dir = os.path.join(\"./data\", dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, \"train.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "val_data = np.memmap(os.path.join(data_dir, \"val.bin\"), dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "print(f'train data length: {len(train_data)}')\n",
    "print(f'val data length: {len(val_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack(\n",
    "        [torch.from_numpy((data[i : i + block_size]).astype(np.int64)) for i in ix]\n",
    "    )\n",
    "    y = torch.stack(\n",
    "        [\n",
    "            torch.from_numpy((data[i + 1 : i + 1 + block_size]).astype(np.int64))\n",
    "            for i in ix\n",
    "        ]\n",
    "    )\n",
    "    if device_type == \"cuda\":\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(\n",
    "            device, non_blocking=True\n",
    "        )\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304  # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.1\n",
      "number of parameters: 123.65M\n",
      "Gradscaler enabled: <bound method GradScaler.is_enabled of <torch.cuda.amp.grad_scaler.GradScaler object at 0x0000020896853D60>>\n",
      "num decayed parameter tensors: 50, with 123,728,640 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# model init\n",
    "model_args = dict(\n",
    "    n_layer=n_layer,\n",
    "    n_head=n_head,\n",
    "    n_embd=n_embd,\n",
    "    block_size=block_size,\n",
    "    bias=bias,\n",
    "    vocab_size=None,\n",
    "    dropout=dropout,\n",
    ")\n",
    "\n",
    "print(f\"Initializing from OpenAI GPT-2 weights: {init_from}\")\n",
    "# initialize from OpenAI GPT-2 weights\n",
    "override_args = dict(dropout=dropout)\n",
    "model = GPT.from_pretrained(init_from, override_args)\n",
    "# read off the created config params, so we can store them into checkpoint correctly\n",
    "for k in [\"n_layer\", \"n_head\", \"n_embd\", \"block_size\", \"bias\", \"vocab_size\"]:\n",
    "        model_args[k] = getattr(model.config, k)\n",
    "        \n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args[\n",
    "        \"block_size\"\n",
    "    ] = block_size  # so that the checkpoint will have the right value\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == \"float16\"))\n",
    "\n",
    "print(f'Gradscaler enabled: {scaler.is_enabled}')\n",
    "\n",
    "# optimizer\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay, learning_rate, (beta1, beta2), device_type\n",
    ")\n",
    "\n",
    "if compile and not isWindowsOS:\n",
    "    print(\"compiling the model... (takes a ~minute)\")\n",
    "    unoptimized_model = model\n",
    "    model = torch.compile(model)  # requires PyTorch 2.0\n",
    "\n",
    "# logging\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    \n",
    "    wandb.init(project=wandb_project, name=wandb_run_name, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 3.8691, val loss 3.4823\n",
      "iter 0: loss 4.5197, time 152703.76ms, mfu -100.00%\n",
      "iter 1: loss 4.2347, time 40571.80ms, mfu -100.00%\n",
      "iter 2: loss 4.0972, time 40575.38ms, mfu -100.00%\n",
      "iter 3: loss 4.1663, time 40589.92ms, mfu -100.00%\n",
      "iter 4: loss 4.0585, time 40588.90ms, mfu -100.00%\n",
      "iter 5: loss 3.5662, time 40582.92ms, mfu -100.00%\n",
      "iter 6: loss 4.4242, time 40645.90ms, mfu -100.00%\n",
      "iter 7: loss 4.3115, time 40596.76ms, mfu -100.00%\n",
      "iter 8: loss 4.1044, time 40648.87ms, mfu -100.00%\n",
      "iter 9: loss 4.0423, time 40629.83ms, mfu -100.00%\n",
      "iter 10: loss 4.1185, time 40595.06ms, mfu -100.00%\n",
      "iter 11: loss 4.4053, time 40666.93ms, mfu -100.00%\n",
      "iter 12: loss 4.5851, time 40696.30ms, mfu -100.00%\n",
      "iter 13: loss 4.0787, time 40649.34ms, mfu -100.00%\n",
      "iter 14: loss 4.1111, time 40651.41ms, mfu -100.00%\n",
      "iter 15: loss 3.8644, time 40683.68ms, mfu -100.00%\n",
      "iter 16: loss 4.3184, time 40662.23ms, mfu -100.00%\n",
      "iter 17: loss 4.0489, time 40642.41ms, mfu -100.00%\n",
      "iter 18: loss 4.0707, time 40693.83ms, mfu -100.00%\n",
      "iter 19: loss 4.0978, time 40629.03ms, mfu -100.00%\n",
      "iter 20: loss 4.0660, time 40584.83ms, mfu -100.00%\n",
      "iter 21: loss 4.0199, time 40604.90ms, mfu -100.00%\n",
      "iter 22: loss 3.8891, time 40506.71ms, mfu -100.00%\n",
      "iter 23: loss 3.2931, time 40623.52ms, mfu -100.00%\n",
      "iter 24: loss 3.9158, time 40633.58ms, mfu -100.00%\n",
      "iter 25: loss 3.1994, time 40660.13ms, mfu -100.00%\n",
      "iter 26: loss 3.8187, time 40645.49ms, mfu -100.00%\n",
      "iter 27: loss 3.7908, time 40632.95ms, mfu -100.00%\n",
      "iter 28: loss 3.4323, time 40624.21ms, mfu -100.00%\n",
      "iter 29: loss 4.1526, time 40624.88ms, mfu -100.00%\n",
      "iter 30: loss 3.8785, time 40735.20ms, mfu -100.00%\n",
      "iter 31: loss 3.8685, time 40667.75ms, mfu -100.00%\n",
      "iter 32: loss 3.8138, time 40692.41ms, mfu -100.00%\n",
      "iter 33: loss 3.7989, time 40641.25ms, mfu -100.00%\n",
      "iter 34: loss 3.7907, time 40628.54ms, mfu -100.00%\n",
      "iter 35: loss 3.9640, time 40678.29ms, mfu -100.00%\n",
      "iter 36: loss 3.7271, time 40623.07ms, mfu -100.00%\n",
      "iter 37: loss 3.7057, time 40621.37ms, mfu -100.00%\n",
      "iter 38: loss 3.7151, time 40611.20ms, mfu -100.00%\n",
      "iter 39: loss 3.6527, time 40686.05ms, mfu -100.00%\n",
      "iter 40: loss 3.7493, time 40645.29ms, mfu -100.00%\n",
      "iter 41: loss 3.7454, time 40625.64ms, mfu -100.00%\n",
      "iter 42: loss 3.2711, time 40678.88ms, mfu -100.00%\n",
      "iter 43: loss 3.3832, time 40697.02ms, mfu -100.00%\n",
      "iter 44: loss 3.7686, time 40626.52ms, mfu -100.00%\n",
      "iter 45: loss 3.5950, time 40635.92ms, mfu -100.00%\n",
      "iter 46: loss 3.3899, time 40665.28ms, mfu -100.00%\n",
      "iter 47: loss 3.6834, time 40642.83ms, mfu -100.00%\n",
      "iter 48: loss 3.5945, time 40695.65ms, mfu -100.00%\n",
      "iter 49: loss 3.6845, time 40606.37ms, mfu -100.00%\n",
      "step 50: train loss 3.4062, val loss 3.1516\n",
      "saving checkpoint to out\n",
      "iter 50: loss 3.6554, time 193252.11ms, mfu -100.00%\n",
      "iter 51: loss 3.5854, time 40602.78ms, mfu -100.00%\n",
      "iter 52: loss 3.5846, time 40665.21ms, mfu -100.00%\n",
      "iter 53: loss 3.6622, time 40693.21ms, mfu -100.00%\n",
      "iter 54: loss 3.8008, time 40706.38ms, mfu -100.00%\n",
      "iter 55: loss 3.5297, time 40713.35ms, mfu -100.00%\n",
      "iter 56: loss 3.5472, time 40689.05ms, mfu -100.00%\n",
      "iter 57: loss 3.8280, time 40696.51ms, mfu -100.00%\n",
      "iter 58: loss 3.7393, time 40688.80ms, mfu -100.00%\n",
      "iter 59: loss 3.6297, time 40724.86ms, mfu -100.00%\n",
      "iter 60: loss 3.3748, time 40725.93ms, mfu -100.00%\n",
      "iter 61: loss 3.5143, time 40689.99ms, mfu -100.00%\n",
      "iter 62: loss 3.2545, time 40572.34ms, mfu -100.00%\n",
      "iter 63: loss 3.5694, time 40649.38ms, mfu -100.00%\n",
      "iter 64: loss 3.5555, time 40686.78ms, mfu -100.00%\n",
      "iter 65: loss 3.3788, time 40672.68ms, mfu -100.00%\n",
      "iter 66: loss 3.6414, time 40716.50ms, mfu -100.00%\n",
      "iter 67: loss 3.8503, time 40740.52ms, mfu -100.00%\n",
      "iter 68: loss 3.3331, time 40705.82ms, mfu -100.00%\n",
      "iter 69: loss 3.5506, time 40716.32ms, mfu -100.00%\n",
      "iter 70: loss 2.6732, time 40677.30ms, mfu -100.00%\n",
      "iter 71: loss 3.4432, time 40675.86ms, mfu -100.00%\n",
      "iter 72: loss 3.6416, time 40725.15ms, mfu -100.00%\n",
      "iter 73: loss 3.3513, time 40711.35ms, mfu -100.00%\n",
      "iter 74: loss 3.4494, time 40681.12ms, mfu -100.00%\n",
      "iter 75: loss 3.5129, time 40685.25ms, mfu -100.00%\n",
      "iter 76: loss 3.3712, time 40724.65ms, mfu -100.00%\n",
      "iter 77: loss 3.4738, time 40726.81ms, mfu -100.00%\n",
      "iter 78: loss 3.4567, time 40721.62ms, mfu -100.00%\n",
      "iter 79: loss 3.2081, time 40709.93ms, mfu -100.00%\n",
      "iter 80: loss 3.3638, time 40723.66ms, mfu -100.00%\n",
      "iter 81: loss 3.0838, time 40679.46ms, mfu -100.00%\n",
      "iter 82: loss 3.5122, time 40675.38ms, mfu -100.00%\n",
      "iter 83: loss 3.3072, time 40668.67ms, mfu -100.00%\n",
      "iter 84: loss 3.3206, time 40732.72ms, mfu -100.00%\n",
      "iter 85: loss 3.3535, time 40698.48ms, mfu -100.00%\n",
      "iter 86: loss 3.1686, time 40779.47ms, mfu -100.00%\n",
      "iter 87: loss 3.3406, time 40698.89ms, mfu -100.00%\n",
      "iter 88: loss 3.1901, time 40740.60ms, mfu -100.00%\n",
      "iter 89: loss 3.3588, time 40763.63ms, mfu -100.00%\n",
      "iter 90: loss 2.8009, time 40731.85ms, mfu -100.00%\n",
      "iter 91: loss 3.1693, time 40707.21ms, mfu -100.00%\n",
      "iter 92: loss 3.3681, time 40760.62ms, mfu -100.00%\n",
      "iter 93: loss 3.3296, time 40754.58ms, mfu -100.00%\n",
      "iter 94: loss 3.3126, time 40719.19ms, mfu -100.00%\n",
      "iter 95: loss 3.3008, time 40761.59ms, mfu -100.00%\n",
      "iter 96: loss 3.2385, time 40818.62ms, mfu -100.00%\n",
      "iter 97: loss 3.4602, time 40793.20ms, mfu -100.00%\n",
      "iter 98: loss 3.1701, time 40806.12ms, mfu -100.00%\n",
      "iter 99: loss 3.1709, time 40841.16ms, mfu -100.00%\n",
      "step 100: train loss 3.0317, val loss 3.0769\n",
      "saving checkpoint to out\n",
      "iter 100: loss 3.2717, time 193854.83ms, mfu -100.00%\n",
      "iter 101: loss 3.0403, time 40711.76ms, mfu -100.00%\n",
      "iter 102: loss 3.1016, time 40695.32ms, mfu -100.00%\n",
      "iter 103: loss 3.1358, time 40755.48ms, mfu -100.00%\n",
      "iter 104: loss 3.1527, time 40764.64ms, mfu -100.00%\n",
      "iter 105: loss 3.1638, time 40733.77ms, mfu -100.00%\n",
      "iter 106: loss 3.1012, time 40850.16ms, mfu -100.00%\n",
      "iter 107: loss 3.0130, time 40748.00ms, mfu -100.00%\n",
      "iter 108: loss 3.0889, time 40798.89ms, mfu -100.00%\n",
      "iter 109: loss 3.0971, time 40768.26ms, mfu -100.00%\n",
      "iter 110: loss 3.1110, time 40779.50ms, mfu -100.00%\n",
      "iter 111: loss 2.9619, time 40739.26ms, mfu -100.00%\n",
      "iter 112: loss 3.0325, time 40775.60ms, mfu -100.00%\n",
      "iter 113: loss 3.1726, time 40760.46ms, mfu -100.00%\n",
      "iter 114: loss 3.0764, time 40751.32ms, mfu -100.00%\n",
      "iter 115: loss 2.8433, time 40824.05ms, mfu -100.00%\n",
      "iter 116: loss 3.1532, time 40723.68ms, mfu -100.00%\n",
      "iter 117: loss 3.1875, time 40761.46ms, mfu -100.00%\n",
      "iter 118: loss 3.1450, time 40811.38ms, mfu -100.00%\n",
      "iter 119: loss 3.3066, time 40814.47ms, mfu -100.00%\n",
      "iter 120: loss 3.1295, time 40767.25ms, mfu -100.00%\n",
      "iter 121: loss 3.1133, time 40742.39ms, mfu -100.00%\n",
      "iter 122: loss 3.1074, time 40740.92ms, mfu -100.00%\n",
      "iter 123: loss 3.1012, time 40714.46ms, mfu -100.00%\n",
      "iter 124: loss 3.1743, time 40744.48ms, mfu -100.00%\n",
      "iter 125: loss 2.9462, time 40732.61ms, mfu -100.00%\n",
      "iter 126: loss 2.9926, time 40736.05ms, mfu -100.00%\n",
      "iter 127: loss 3.0336, time 40768.02ms, mfu -100.00%\n",
      "iter 128: loss 2.9577, time 40736.84ms, mfu -100.00%\n",
      "iter 129: loss 3.2101, time 40734.97ms, mfu -100.00%\n",
      "iter 130: loss 3.0208, time 40849.94ms, mfu -100.00%\n",
      "iter 131: loss 2.9291, time 40778.49ms, mfu -100.00%\n",
      "iter 132: loss 3.0330, time 40757.59ms, mfu -100.00%\n",
      "iter 133: loss 2.9130, time 40743.56ms, mfu -100.00%\n",
      "iter 134: loss 3.0492, time 40723.89ms, mfu -100.00%\n",
      "iter 135: loss 2.9996, time 40680.12ms, mfu -100.00%\n",
      "iter 136: loss 3.0555, time 40728.38ms, mfu -100.00%\n",
      "iter 137: loss 2.9812, time 40704.45ms, mfu -100.00%\n",
      "iter 138: loss 3.0155, time 40730.13ms, mfu -100.00%\n",
      "iter 139: loss 3.1429, time 40765.37ms, mfu -100.00%\n",
      "iter 140: loss 2.9512, time 40836.37ms, mfu -100.00%\n",
      "iter 141: loss 3.1223, time 40741.74ms, mfu -100.00%\n",
      "iter 142: loss 2.9313, time 40802.10ms, mfu -100.00%\n",
      "iter 143: loss 3.0137, time 40775.18ms, mfu -100.00%\n",
      "iter 144: loss 3.1639, time 40820.04ms, mfu -100.00%\n",
      "iter 145: loss 3.0624, time 40728.73ms, mfu -100.00%\n",
      "iter 146: loss 3.1247, time 40765.87ms, mfu -100.00%\n",
      "iter 147: loss 2.9758, time 40802.35ms, mfu -100.00%\n",
      "iter 148: loss 2.9703, time 40755.23ms, mfu -100.00%\n",
      "iter 149: loss 2.8601, time 40773.07ms, mfu -100.00%\n",
      "step 150: train loss 2.8198, val loss 3.0086\n",
      "saving checkpoint to out\n",
      "iter 150: loss 2.9699, time 194064.09ms, mfu -100.00%\n",
      "iter 151: loss 3.0750, time 40729.00ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 152: loss 2.7737, time 40711.25ms, mfu -100.00%\n",
      "iter 153: loss 3.1077, time 40830.17ms, mfu -100.00%\n",
      "iter 154: loss 2.7244, time 40738.40ms, mfu -100.00%\n",
      "iter 155: loss 2.6497, time 40744.90ms, mfu -100.00%\n",
      "iter 156: loss 2.9749, time 40768.09ms, mfu -100.00%\n",
      "iter 157: loss 2.7626, time 40767.79ms, mfu -100.00%\n",
      "iter 158: loss 3.0569, time 40753.89ms, mfu -100.00%\n",
      "iter 159: loss 2.8146, time 40776.87ms, mfu -100.00%\n",
      "iter 160: loss 2.9872, time 40756.46ms, mfu -100.00%\n",
      "iter 161: loss 2.7289, time 40754.24ms, mfu -100.00%\n",
      "iter 162: loss 2.6785, time 40765.74ms, mfu -100.00%\n",
      "iter 163: loss 2.8144, time 40843.45ms, mfu -100.00%\n",
      "iter 164: loss 2.5314, time 40710.49ms, mfu -100.00%\n",
      "iter 165: loss 2.8418, time 40734.80ms, mfu -100.00%\n",
      "iter 166: loss 2.7512, time 40781.18ms, mfu -100.00%\n",
      "iter 167: loss 3.1973, time 40743.57ms, mfu -100.00%\n",
      "iter 168: loss 2.8734, time 40777.81ms, mfu -100.00%\n",
      "iter 169: loss 3.0638, time 40805.22ms, mfu -100.00%\n",
      "iter 170: loss 2.8456, time 40831.78ms, mfu -100.00%\n",
      "iter 171: loss 2.9153, time 40727.63ms, mfu -100.00%\n",
      "iter 172: loss 2.8954, time 40717.74ms, mfu -100.00%\n",
      "iter 173: loss 2.5620, time 40772.04ms, mfu -100.00%\n",
      "iter 174: loss 2.8427, time 40818.95ms, mfu -100.00%\n",
      "iter 175: loss 3.0416, time 40810.62ms, mfu -100.00%\n",
      "iter 176: loss 2.9319, time 40779.41ms, mfu -100.00%\n",
      "iter 177: loss 2.7712, time 40731.08ms, mfu -100.00%\n",
      "iter 178: loss 2.8993, time 40769.95ms, mfu -100.00%\n",
      "iter 179: loss 2.6285, time 40736.41ms, mfu -100.00%\n",
      "iter 180: loss 3.0619, time 40809.30ms, mfu -100.00%\n",
      "iter 181: loss 2.6765, time 40755.27ms, mfu -100.00%\n",
      "iter 182: loss 3.0391, time 40766.46ms, mfu -100.00%\n",
      "iter 183: loss 3.0016, time 40722.64ms, mfu -100.00%\n",
      "iter 184: loss 2.5950, time 40747.99ms, mfu -100.00%\n",
      "iter 185: loss 2.6860, time 40783.47ms, mfu -100.00%\n",
      "iter 186: loss 2.6870, time 40763.54ms, mfu -100.00%\n",
      "iter 187: loss 2.8252, time 40761.51ms, mfu -100.00%\n",
      "iter 188: loss 2.8411, time 40781.18ms, mfu -100.00%\n",
      "iter 189: loss 2.7929, time 40756.46ms, mfu -100.00%\n",
      "iter 190: loss 2.7392, time 40730.69ms, mfu -100.00%\n",
      "iter 191: loss 2.9283, time 40791.46ms, mfu -100.00%\n",
      "iter 192: loss 2.3793, time 40770.97ms, mfu -100.00%\n",
      "iter 193: loss 2.7107, time 40729.64ms, mfu -100.00%\n",
      "iter 194: loss 2.7203, time 40767.82ms, mfu -100.00%\n",
      "iter 195: loss 2.8971, time 40739.35ms, mfu -100.00%\n",
      "iter 196: loss 2.7725, time 40749.74ms, mfu -100.00%\n",
      "iter 197: loss 2.7550, time 40737.28ms, mfu -100.00%\n",
      "iter 198: loss 2.6931, time 40771.62ms, mfu -100.00%\n",
      "iter 199: loss 2.7920, time 40712.75ms, mfu -100.00%\n",
      "step 200: train loss 2.6297, val loss 3.0319\n",
      "saving checkpoint to out\n",
      "iter 200: loss 2.8829, time 193787.87ms, mfu -100.00%\n",
      "iter 201: loss 3.0468, time 40647.15ms, mfu -100.00%\n",
      "iter 202: loss 2.7071, time 40717.49ms, mfu -100.00%\n",
      "iter 203: loss 2.9455, time 40720.72ms, mfu -100.00%\n",
      "iter 204: loss 2.9305, time 40714.92ms, mfu -100.00%\n",
      "iter 205: loss 2.6480, time 40766.42ms, mfu -100.00%\n",
      "iter 206: loss 2.8245, time 40714.42ms, mfu -100.00%\n",
      "iter 207: loss 2.8436, time 40767.49ms, mfu -100.00%\n",
      "iter 208: loss 2.8828, time 40780.48ms, mfu -100.00%\n",
      "iter 209: loss 2.6569, time 40746.48ms, mfu -100.00%\n",
      "iter 210: loss 2.8011, time 40740.25ms, mfu -100.00%\n",
      "iter 211: loss 2.9307, time 40829.68ms, mfu -100.00%\n",
      "iter 212: loss 2.9100, time 40637.74ms, mfu -100.00%\n",
      "iter 213: loss 2.7354, time 40612.02ms, mfu -100.00%\n",
      "iter 214: loss 2.3138, time 40796.39ms, mfu -100.00%\n",
      "iter 215: loss 2.7415, time 40725.81ms, mfu -100.00%\n",
      "iter 216: loss 2.9354, time 40830.15ms, mfu -100.00%\n",
      "iter 217: loss 2.6178, time 40742.64ms, mfu -100.00%\n",
      "iter 218: loss 2.8703, time 40737.97ms, mfu -100.00%\n",
      "iter 219: loss 2.7569, time 40752.35ms, mfu -100.00%\n",
      "iter 220: loss 2.6617, time 40740.51ms, mfu -100.00%\n",
      "iter 221: loss 2.7538, time 40734.82ms, mfu -100.00%\n",
      "iter 222: loss 2.6211, time 40760.88ms, mfu -100.00%\n",
      "iter 223: loss 2.7238, time 40784.06ms, mfu -100.00%\n",
      "iter 224: loss 2.6106, time 40811.48ms, mfu -100.00%\n",
      "iter 225: loss 2.9349, time 40731.61ms, mfu -100.00%\n",
      "iter 226: loss 2.8933, time 40752.58ms, mfu -100.00%\n",
      "iter 227: loss 2.6544, time 40754.89ms, mfu -100.00%\n",
      "iter 228: loss 2.7078, time 40773.98ms, mfu -100.00%\n",
      "iter 229: loss 3.0597, time 40755.16ms, mfu -100.00%\n",
      "iter 230: loss 2.7568, time 40735.46ms, mfu -100.00%\n",
      "iter 231: loss 2.6765, time 40803.73ms, mfu -100.00%\n",
      "iter 232: loss 2.8381, time 40787.07ms, mfu -100.00%\n",
      "iter 233: loss 2.9226, time 40764.46ms, mfu -100.00%\n",
      "iter 234: loss 2.8918, time 40700.79ms, mfu -100.00%\n",
      "iter 235: loss 2.5403, time 40752.66ms, mfu -100.00%\n",
      "iter 236: loss 2.6853, time 40733.83ms, mfu -100.00%\n",
      "iter 237: loss 2.6691, time 40807.60ms, mfu -100.00%\n",
      "iter 238: loss 2.9051, time 40735.00ms, mfu -100.00%\n",
      "iter 239: loss 2.7218, time 40752.07ms, mfu -100.00%\n",
      "iter 240: loss 2.7232, time 40779.76ms, mfu -100.00%\n",
      "iter 241: loss 2.9010, time 40811.33ms, mfu -100.00%\n",
      "iter 242: loss 2.5264, time 40729.95ms, mfu -100.00%\n",
      "iter 243: loss 2.5365, time 40726.90ms, mfu -100.00%\n",
      "iter 244: loss 2.6050, time 40718.51ms, mfu -100.00%\n",
      "iter 245: loss 2.6943, time 40730.02ms, mfu -100.00%\n",
      "iter 246: loss 2.4336, time 40755.48ms, mfu -100.00%\n",
      "iter 247: loss 2.6546, time 40749.76ms, mfu -100.00%\n",
      "iter 248: loss 2.8520, time 40682.01ms, mfu -100.00%\n",
      "iter 249: loss 2.5964, time 40730.40ms, mfu -100.00%\n",
      "step 250: train loss 2.4814, val loss 3.0414\n",
      "saving checkpoint to out\n",
      "iter 250: loss 2.6382, time 193478.01ms, mfu -100.00%\n",
      "iter 251: loss 2.5184, time 40644.53ms, mfu -100.00%\n",
      "iter 252: loss 2.4364, time 40594.93ms, mfu -100.00%\n",
      "iter 253: loss 2.7023, time 40692.54ms, mfu -100.00%\n",
      "iter 254: loss 2.5468, time 40697.04ms, mfu -100.00%\n",
      "iter 255: loss 2.6892, time 40658.75ms, mfu -100.00%\n",
      "iter 256: loss 2.6863, time 40706.11ms, mfu -100.00%\n",
      "iter 257: loss 2.7331, time 40701.17ms, mfu -100.00%\n",
      "iter 258: loss 2.4557, time 40702.71ms, mfu -100.00%\n",
      "iter 259: loss 2.5799, time 40724.72ms, mfu -100.00%\n",
      "iter 260: loss 2.6912, time 40783.66ms, mfu -100.00%\n",
      "iter 261: loss 2.5735, time 40774.48ms, mfu -100.00%\n",
      "iter 262: loss 2.6899, time 40779.80ms, mfu -100.00%\n",
      "iter 263: loss 2.6817, time 40668.40ms, mfu -100.00%\n",
      "iter 264: loss 2.5954, time 40734.22ms, mfu -100.00%\n",
      "iter 265: loss 2.7111, time 40616.05ms, mfu -100.00%\n",
      "iter 266: loss 2.4141, time 40732.70ms, mfu -100.00%\n",
      "iter 267: loss 2.0285, time 40792.45ms, mfu -100.00%\n",
      "iter 268: loss 2.3688, time 40779.46ms, mfu -100.00%\n",
      "iter 269: loss 2.4796, time 40730.20ms, mfu -100.00%\n",
      "iter 270: loss 2.7003, time 40705.55ms, mfu -100.00%\n",
      "iter 271: loss 2.7603, time 40595.68ms, mfu -100.00%\n",
      "iter 272: loss 2.5627, time 40695.24ms, mfu -100.00%\n",
      "iter 273: loss 2.5971, time 40577.51ms, mfu -100.00%\n",
      "iter 274: loss 2.4101, time 40579.55ms, mfu -100.00%\n",
      "iter 275: loss 2.6319, time 40622.25ms, mfu -100.00%\n",
      "iter 276: loss 2.6860, time 40602.65ms, mfu -100.00%\n",
      "iter 277: loss 2.2957, time 40647.26ms, mfu -100.00%\n",
      "iter 278: loss 2.4355, time 40597.09ms, mfu -100.00%\n",
      "iter 279: loss 2.3855, time 40744.89ms, mfu -100.00%\n",
      "iter 280: loss 2.4092, time 40719.62ms, mfu -100.00%\n",
      "iter 281: loss 2.7028, time 40708.34ms, mfu -100.00%\n",
      "iter 282: loss 2.5116, time 40694.88ms, mfu -100.00%\n",
      "iter 283: loss 2.7319, time 40747.80ms, mfu -100.00%\n",
      "iter 284: loss 2.6905, time 40746.94ms, mfu -100.00%\n",
      "iter 285: loss 2.5125, time 40721.49ms, mfu -100.00%\n",
      "iter 286: loss 2.4949, time 40799.93ms, mfu -100.00%\n",
      "iter 287: loss 2.4612, time 40773.74ms, mfu -100.00%\n",
      "iter 288: loss 2.4115, time 40689.50ms, mfu -100.00%\n",
      "iter 289: loss 2.5648, time 40740.99ms, mfu -100.00%\n",
      "iter 290: loss 2.5001, time 40775.17ms, mfu -100.00%\n",
      "iter 291: loss 2.3803, time 40638.00ms, mfu -100.00%\n",
      "iter 292: loss 2.4734, time 40693.90ms, mfu -100.00%\n",
      "iter 293: loss 2.4920, time 40636.00ms, mfu -100.00%\n",
      "iter 294: loss 2.4063, time 40699.49ms, mfu -100.00%\n",
      "iter 295: loss 2.4856, time 40634.65ms, mfu -100.00%\n",
      "iter 296: loss 2.5709, time 40714.11ms, mfu -100.00%\n",
      "iter 297: loss 2.4368, time 40747.94ms, mfu -100.00%\n",
      "iter 298: loss 2.6224, time 40780.97ms, mfu -100.00%\n",
      "iter 299: loss 2.5121, time 40814.30ms, mfu -100.00%\n",
      "step 300: train loss 2.3365, val loss 3.0679\n",
      "saving checkpoint to out\n",
      "iter 300: loss 2.7115, time 193872.50ms, mfu -100.00%\n",
      "iter 301: loss 2.5254, time 40712.89ms, mfu -100.00%\n",
      "iter 302: loss 2.3944, time 40741.33ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 303: loss 2.2213, time 40725.05ms, mfu -100.00%\n",
      "iter 304: loss 2.4158, time 40713.91ms, mfu -100.00%\n",
      "iter 305: loss 2.4905, time 40800.81ms, mfu -100.00%\n",
      "iter 306: loss 2.5885, time 40854.00ms, mfu -100.00%\n",
      "iter 307: loss 2.4223, time 40717.55ms, mfu -100.00%\n",
      "iter 308: loss 2.4857, time 40760.38ms, mfu -100.00%\n",
      "iter 309: loss 2.6738, time 40689.67ms, mfu -100.00%\n",
      "iter 310: loss 2.4517, time 40734.31ms, mfu -100.00%\n",
      "iter 311: loss 2.5622, time 40797.37ms, mfu -100.00%\n",
      "iter 312: loss 2.7362, time 40799.16ms, mfu -100.00%\n",
      "iter 313: loss 2.8516, time 40765.53ms, mfu -100.00%\n",
      "iter 314: loss 2.5169, time 40811.18ms, mfu -100.00%\n",
      "iter 315: loss 2.2502, time 40693.85ms, mfu -100.00%\n",
      "iter 316: loss 2.2824, time 40728.14ms, mfu -100.00%\n",
      "iter 317: loss 2.4637, time 40688.23ms, mfu -100.00%\n",
      "iter 318: loss 2.4263, time 40659.56ms, mfu -100.00%\n",
      "iter 319: loss 2.5210, time 40633.00ms, mfu -100.00%\n",
      "iter 320: loss 2.4014, time 40670.79ms, mfu -100.00%\n",
      "iter 321: loss 2.5378, time 40670.00ms, mfu -100.00%\n",
      "iter 322: loss 2.3146, time 40671.43ms, mfu -100.00%\n",
      "iter 323: loss 2.5066, time 40636.43ms, mfu -100.00%\n",
      "iter 324: loss 2.5372, time 40721.00ms, mfu -100.00%\n",
      "iter 325: loss 2.4652, time 40754.54ms, mfu -100.00%\n",
      "iter 326: loss 2.4776, time 40835.96ms, mfu -100.00%\n",
      "iter 327: loss 2.4146, time 40873.99ms, mfu -100.00%\n",
      "iter 328: loss 2.4683, time 40845.30ms, mfu -100.00%\n",
      "iter 329: loss 2.4579, time 40791.15ms, mfu -100.00%\n",
      "iter 330: loss 2.5442, time 40811.69ms, mfu -100.00%\n",
      "iter 331: loss 2.5358, time 40812.65ms, mfu -100.00%\n",
      "iter 332: loss 2.3299, time 40832.77ms, mfu -100.00%\n",
      "iter 333: loss 2.4220, time 40829.79ms, mfu -100.00%\n",
      "iter 334: loss 2.5950, time 40775.96ms, mfu -100.00%\n",
      "iter 335: loss 2.6448, time 40819.27ms, mfu -100.00%\n",
      "iter 336: loss 2.3005, time 40797.72ms, mfu -100.00%\n",
      "iter 337: loss 2.6573, time 40797.12ms, mfu -100.00%\n",
      "iter 338: loss 2.3156, time 40840.10ms, mfu -100.00%\n",
      "iter 339: loss 2.3727, time 40739.78ms, mfu -100.00%\n",
      "iter 340: loss 2.2700, time 40814.45ms, mfu -100.00%\n",
      "iter 341: loss 2.3491, time 40800.29ms, mfu -100.00%\n",
      "iter 342: loss 2.1645, time 40823.28ms, mfu -100.00%\n",
      "iter 343: loss 2.1257, time 40811.90ms, mfu -100.00%\n",
      "iter 344: loss 2.2050, time 40835.99ms, mfu -100.00%\n",
      "iter 345: loss 2.3410, time 40791.19ms, mfu -100.00%\n",
      "iter 346: loss 2.6005, time 40769.10ms, mfu -100.00%\n",
      "iter 347: loss 2.1758, time 40773.28ms, mfu -100.00%\n",
      "iter 348: loss 2.2400, time 40751.96ms, mfu -100.00%\n",
      "iter 349: loss 2.3985, time 40783.35ms, mfu -100.00%\n",
      "step 350: train loss 2.2035, val loss 3.1121\n",
      "saving checkpoint to out\n",
      "iter 350: loss 2.5580, time 193534.81ms, mfu -100.00%\n",
      "iter 351: loss 2.3235, time 40634.00ms, mfu -100.00%\n",
      "iter 352: loss 2.4557, time 40725.31ms, mfu -100.00%\n",
      "iter 353: loss 2.3845, time 40711.30ms, mfu -100.00%\n",
      "iter 354: loss 2.0804, time 40762.89ms, mfu -100.00%\n",
      "iter 355: loss 1.9869, time 40784.86ms, mfu -100.00%\n",
      "iter 356: loss 2.4501, time 40696.69ms, mfu -100.00%\n",
      "iter 357: loss 2.5505, time 40721.29ms, mfu -100.00%\n",
      "iter 358: loss 2.3617, time 40791.75ms, mfu -100.00%\n",
      "iter 359: loss 2.2230, time 40709.31ms, mfu -100.00%\n",
      "iter 360: loss 2.1286, time 40821.76ms, mfu -100.00%\n",
      "iter 361: loss 2.3334, time 40781.36ms, mfu -100.00%\n",
      "iter 362: loss 2.1894, time 40744.59ms, mfu -100.00%\n",
      "iter 363: loss 2.3767, time 40758.87ms, mfu -100.00%\n",
      "iter 364: loss 2.7214, time 40746.85ms, mfu -100.00%\n",
      "iter 365: loss 2.3449, time 40705.41ms, mfu -100.00%\n",
      "iter 366: loss 2.5181, time 40755.50ms, mfu -100.00%\n",
      "iter 367: loss 2.0044, time 40775.70ms, mfu -100.00%\n",
      "iter 368: loss 2.4393, time 40817.08ms, mfu -100.00%\n",
      "iter 369: loss 2.3456, time 40671.70ms, mfu -100.00%\n",
      "iter 370: loss 2.2554, time 40723.41ms, mfu -100.00%\n",
      "iter 371: loss 2.2341, time 40711.32ms, mfu -100.00%\n",
      "iter 372: loss 2.3180, time 40721.71ms, mfu -100.00%\n",
      "iter 373: loss 2.2673, time 40692.38ms, mfu -100.00%\n",
      "iter 374: loss 2.3447, time 40730.95ms, mfu -100.00%\n",
      "iter 375: loss 2.2533, time 40709.79ms, mfu -100.00%\n",
      "iter 376: loss 2.3162, time 40742.20ms, mfu -100.00%\n",
      "iter 377: loss 2.4327, time 40751.18ms, mfu -100.00%\n",
      "iter 378: loss 2.1973, time 40712.36ms, mfu -100.00%\n",
      "iter 379: loss 2.0887, time 40733.21ms, mfu -100.00%\n",
      "iter 380: loss 2.3264, time 40746.35ms, mfu -100.00%\n",
      "iter 381: loss 2.2403, time 40704.79ms, mfu -100.00%\n",
      "iter 382: loss 2.4271, time 40751.24ms, mfu -100.00%\n",
      "iter 383: loss 2.3193, time 40784.90ms, mfu -100.00%\n",
      "iter 384: loss 2.0663, time 40770.78ms, mfu -100.00%\n",
      "iter 385: loss 2.3317, time 40761.04ms, mfu -100.00%\n",
      "iter 386: loss 2.2697, time 40772.46ms, mfu -100.00%\n",
      "iter 387: loss 2.2433, time 40710.65ms, mfu -100.00%\n",
      "iter 388: loss 2.2152, time 40760.83ms, mfu -100.00%\n",
      "iter 389: loss 2.0844, time 40770.30ms, mfu -100.00%\n",
      "iter 390: loss 2.2193, time 40745.03ms, mfu -100.00%\n",
      "iter 391: loss 2.3245, time 40766.70ms, mfu -100.00%\n",
      "iter 392: loss 2.3983, time 40735.00ms, mfu -100.00%\n",
      "iter 393: loss 2.5021, time 40738.87ms, mfu -100.00%\n",
      "iter 394: loss 2.4061, time 40842.33ms, mfu -100.00%\n",
      "iter 395: loss 2.2023, time 40728.54ms, mfu -100.00%\n",
      "iter 396: loss 2.2107, time 40730.04ms, mfu -100.00%\n",
      "iter 397: loss 2.3595, time 40768.54ms, mfu -100.00%\n",
      "iter 398: loss 2.5249, time 40831.64ms, mfu -100.00%\n",
      "iter 399: loss 2.2467, time 40788.34ms, mfu -100.00%\n",
      "step 400: train loss 2.0463, val loss 3.2299\n",
      "saving checkpoint to out\n",
      "iter 400: loss 2.2890, time 194135.13ms, mfu -100.00%\n",
      "iter 401: loss 2.1313, time 40673.39ms, mfu -100.00%\n",
      "iter 402: loss 2.2450, time 40714.68ms, mfu -100.00%\n",
      "iter 403: loss 2.3943, time 40728.17ms, mfu -100.00%\n",
      "iter 404: loss 2.1812, time 40760.98ms, mfu -100.00%\n",
      "iter 405: loss 1.9198, time 40734.67ms, mfu -100.00%\n",
      "iter 406: loss 2.3233, time 40705.10ms, mfu -100.00%\n",
      "iter 407: loss 2.1210, time 40682.47ms, mfu -100.00%\n",
      "iter 408: loss 2.4349, time 40790.46ms, mfu -100.00%\n",
      "iter 409: loss 2.2061, time 40803.35ms, mfu -100.00%\n",
      "iter 410: loss 2.3576, time 40673.58ms, mfu -100.00%\n",
      "iter 411: loss 2.2944, time 40836.29ms, mfu -100.00%\n",
      "iter 412: loss 2.3085, time 40705.64ms, mfu -100.00%\n",
      "iter 413: loss 2.2870, time 40750.15ms, mfu -100.00%\n",
      "iter 414: loss 2.3636, time 40831.67ms, mfu -100.00%\n",
      "iter 415: loss 2.2243, time 40630.07ms, mfu -100.00%\n",
      "iter 416: loss 2.0909, time 40671.13ms, mfu -100.00%\n",
      "iter 417: loss 1.9155, time 40688.07ms, mfu -100.00%\n",
      "iter 418: loss 2.3546, time 40697.20ms, mfu -100.00%\n",
      "iter 419: loss 2.0181, time 40809.23ms, mfu -100.00%\n",
      "iter 420: loss 2.3402, time 40787.58ms, mfu -100.00%\n",
      "iter 421: loss 1.9416, time 40675.68ms, mfu -100.00%\n",
      "iter 422: loss 2.3285, time 40773.43ms, mfu -100.00%\n",
      "iter 423: loss 2.1331, time 40730.07ms, mfu -100.00%\n",
      "iter 424: loss 2.2471, time 40739.57ms, mfu -100.00%\n",
      "iter 425: loss 2.2953, time 40716.31ms, mfu -100.00%\n",
      "iter 426: loss 1.9475, time 40759.08ms, mfu -100.00%\n",
      "iter 427: loss 2.2085, time 40663.46ms, mfu -100.00%\n",
      "iter 428: loss 2.2811, time 40648.73ms, mfu -100.00%\n",
      "iter 429: loss 2.5474, time 40833.79ms, mfu -100.00%\n",
      "iter 430: loss 2.4060, time 40721.35ms, mfu -100.00%\n",
      "iter 431: loss 1.8188, time 40799.05ms, mfu -100.00%\n",
      "iter 432: loss 2.1875, time 40807.99ms, mfu -100.00%\n",
      "iter 433: loss 2.2460, time 40742.58ms, mfu -100.00%\n",
      "iter 434: loss 2.2574, time 40778.56ms, mfu -100.00%\n",
      "iter 435: loss 2.0610, time 40740.16ms, mfu -100.00%\n",
      "iter 436: loss 2.3953, time 40711.19ms, mfu -100.00%\n",
      "iter 437: loss 2.2130, time 40755.51ms, mfu -100.00%\n",
      "iter 438: loss 2.0115, time 40834.57ms, mfu -100.00%\n",
      "iter 439: loss 2.1353, time 40772.50ms, mfu -100.00%\n",
      "iter 440: loss 2.1730, time 40821.20ms, mfu -100.00%\n",
      "iter 441: loss 2.1526, time 40716.53ms, mfu -100.00%\n",
      "iter 442: loss 2.1015, time 40745.59ms, mfu -100.00%\n",
      "iter 443: loss 2.1025, time 40735.14ms, mfu -100.00%\n",
      "iter 444: loss 2.2987, time 40711.24ms, mfu -100.00%\n",
      "iter 445: loss 2.1366, time 40692.42ms, mfu -100.00%\n",
      "iter 446: loss 2.0397, time 40732.99ms, mfu -100.00%\n",
      "iter 447: loss 1.7446, time 40719.69ms, mfu -100.00%\n",
      "iter 448: loss 2.1833, time 40717.21ms, mfu -100.00%\n",
      "iter 449: loss 2.1184, time 40775.04ms, mfu -100.00%\n",
      "step 450: train loss 1.8742, val loss 3.2811\n",
      "saving checkpoint to out\n",
      "iter 450: loss 2.0487, time 194002.41ms, mfu -100.00%\n",
      "iter 451: loss 2.2275, time 40660.61ms, mfu -100.00%\n",
      "iter 452: loss 2.1482, time 40630.32ms, mfu -100.00%\n",
      "iter 453: loss 2.0789, time 40663.55ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 454: loss 2.1375, time 40677.10ms, mfu -100.00%\n",
      "iter 455: loss 2.2053, time 40699.10ms, mfu -100.00%\n",
      "iter 456: loss 2.4016, time 40724.15ms, mfu -100.00%\n",
      "iter 457: loss 2.0805, time 40739.62ms, mfu -100.00%\n",
      "iter 458: loss 1.8967, time 40708.60ms, mfu -100.00%\n",
      "iter 459: loss 2.1392, time 40806.30ms, mfu -100.00%\n",
      "iter 460: loss 2.1038, time 40762.41ms, mfu -100.00%\n",
      "iter 461: loss 2.1134, time 40741.31ms, mfu -100.00%\n",
      "iter 462: loss 1.9972, time 40754.82ms, mfu -100.00%\n",
      "iter 463: loss 2.1489, time 40745.40ms, mfu -100.00%\n",
      "iter 464: loss 2.1769, time 40701.38ms, mfu -100.00%\n",
      "iter 465: loss 2.0766, time 40742.48ms, mfu -100.00%\n",
      "iter 466: loss 1.8356, time 40758.79ms, mfu -100.00%\n",
      "iter 467: loss 2.1532, time 40733.38ms, mfu -100.00%\n",
      "iter 468: loss 2.0430, time 40735.96ms, mfu -100.00%\n",
      "iter 469: loss 2.2716, time 40729.93ms, mfu -100.00%\n",
      "iter 470: loss 1.9657, time 40684.16ms, mfu -100.00%\n",
      "iter 471: loss 1.9041, time 40720.71ms, mfu -100.00%\n",
      "iter 472: loss 1.9053, time 40744.12ms, mfu -100.00%\n",
      "iter 473: loss 2.1272, time 40750.04ms, mfu -100.00%\n",
      "iter 474: loss 2.0745, time 40759.24ms, mfu -100.00%\n",
      "iter 475: loss 2.1929, time 40773.31ms, mfu -100.00%\n",
      "iter 476: loss 2.0745, time 40756.24ms, mfu -100.00%\n",
      "iter 477: loss 2.2210, time 40712.36ms, mfu -100.00%\n",
      "iter 478: loss 2.2061, time 40745.85ms, mfu -100.00%\n",
      "iter 479: loss 2.0012, time 40652.02ms, mfu -100.00%\n",
      "iter 480: loss 1.9562, time 40733.84ms, mfu -100.00%\n",
      "iter 481: loss 2.0355, time 40771.06ms, mfu -100.00%\n",
      "iter 482: loss 2.1650, time 40723.33ms, mfu -100.00%\n",
      "iter 483: loss 1.9144, time 40706.68ms, mfu -100.00%\n",
      "iter 484: loss 2.0969, time 40799.61ms, mfu -100.00%\n",
      "iter 485: loss 1.8833, time 40679.44ms, mfu -100.00%\n",
      "iter 486: loss 2.0263, time 40666.58ms, mfu -100.00%\n",
      "iter 487: loss 2.1236, time 40712.33ms, mfu -100.00%\n",
      "iter 488: loss 2.0646, time 40718.33ms, mfu -100.00%\n",
      "iter 489: loss 1.9679, time 40751.45ms, mfu -100.00%\n",
      "iter 490: loss 1.9345, time 40684.93ms, mfu -100.00%\n",
      "iter 491: loss 2.0959, time 40699.69ms, mfu -100.00%\n",
      "iter 492: loss 1.9474, time 40750.19ms, mfu -100.00%\n",
      "iter 493: loss 2.1145, time 40657.73ms, mfu -100.00%\n",
      "iter 494: loss 2.1207, time 40621.59ms, mfu -100.00%\n",
      "iter 495: loss 1.8997, time 40717.07ms, mfu -100.00%\n",
      "iter 496: loss 2.0223, time 40691.78ms, mfu -100.00%\n",
      "iter 497: loss 1.9915, time 40608.36ms, mfu -100.00%\n",
      "iter 498: loss 1.7267, time 40628.88ms, mfu -100.00%\n",
      "iter 499: loss 1.8776, time 40715.37ms, mfu -100.00%\n",
      "step 500: train loss 1.7267, val loss 3.3257\n",
      "saving checkpoint to out\n",
      "iter 500: loss 1.9646, time 193851.27ms, mfu -100.00%\n",
      "iter 501: loss 1.9453, time 40756.07ms, mfu -100.00%\n",
      "iter 502: loss 1.9280, time 40691.63ms, mfu -100.00%\n",
      "iter 503: loss 2.0046, time 40714.51ms, mfu -100.00%\n",
      "iter 504: loss 1.9941, time 40785.76ms, mfu -100.00%\n",
      "iter 505: loss 1.9741, time 40712.84ms, mfu -100.00%\n",
      "iter 506: loss 1.8015, time 40757.25ms, mfu -100.00%\n",
      "iter 507: loss 1.8898, time 40749.22ms, mfu -100.00%\n",
      "iter 508: loss 1.7051, time 40728.06ms, mfu -100.00%\n",
      "iter 509: loss 1.9840, time 40777.87ms, mfu -100.00%\n",
      "iter 510: loss 1.7719, time 40800.20ms, mfu -100.00%\n",
      "iter 511: loss 1.7667, time 40686.99ms, mfu -100.00%\n",
      "iter 512: loss 1.7550, time 40778.26ms, mfu -100.00%\n",
      "iter 513: loss 2.0565, time 40822.19ms, mfu -100.00%\n",
      "iter 514: loss 2.1624, time 40713.19ms, mfu -100.00%\n",
      "iter 515: loss 1.8994, time 40701.90ms, mfu -100.00%\n",
      "iter 516: loss 1.9534, time 40760.19ms, mfu -100.00%\n",
      "iter 517: loss 1.8831, time 40719.09ms, mfu -100.00%\n",
      "iter 518: loss 1.8400, time 40735.47ms, mfu -100.00%\n",
      "iter 519: loss 1.9471, time 40716.24ms, mfu -100.00%\n",
      "iter 520: loss 1.8309, time 40746.33ms, mfu -100.00%\n",
      "iter 521: loss 1.8502, time 40755.80ms, mfu -100.00%\n",
      "iter 522: loss 1.9532, time 40741.44ms, mfu -100.00%\n",
      "iter 523: loss 1.6689, time 40745.76ms, mfu -100.00%\n",
      "iter 524: loss 1.8061, time 40815.29ms, mfu -100.00%\n",
      "iter 525: loss 1.7381, time 40736.16ms, mfu -100.00%\n",
      "iter 526: loss 1.9696, time 40732.12ms, mfu -100.00%\n",
      "iter 527: loss 1.8795, time 40725.18ms, mfu -100.00%\n",
      "iter 528: loss 1.7773, time 40758.96ms, mfu -100.00%\n",
      "iter 529: loss 1.8689, time 40722.54ms, mfu -100.00%\n",
      "iter 530: loss 1.9552, time 40760.79ms, mfu -100.00%\n",
      "iter 531: loss 1.8375, time 40715.12ms, mfu -100.00%\n",
      "iter 532: loss 1.8672, time 40714.37ms, mfu -100.00%\n",
      "iter 533: loss 2.0177, time 40742.39ms, mfu -100.00%\n",
      "iter 534: loss 1.7981, time 40826.02ms, mfu -100.00%\n",
      "iter 535: loss 2.0247, time 40757.18ms, mfu -100.00%\n",
      "iter 536: loss 1.1215, time 40786.66ms, mfu -100.00%\n",
      "iter 537: loss 2.1375, time 40735.85ms, mfu -100.00%\n",
      "iter 538: loss 1.7585, time 40704.87ms, mfu -100.00%\n",
      "iter 539: loss 1.6445, time 40729.47ms, mfu -100.00%\n",
      "iter 540: loss 1.6847, time 40703.47ms, mfu -100.00%\n",
      "iter 541: loss 1.8469, time 40728.82ms, mfu -100.00%\n",
      "iter 542: loss 1.7758, time 40736.56ms, mfu -100.00%\n",
      "iter 543: loss 1.5327, time 40720.78ms, mfu -100.00%\n",
      "iter 544: loss 1.8654, time 40703.73ms, mfu -100.00%\n",
      "iter 545: loss 1.6604, time 40789.42ms, mfu -100.00%\n",
      "iter 546: loss 2.0870, time 40695.38ms, mfu -100.00%\n",
      "iter 547: loss 2.1591, time 40770.58ms, mfu -100.00%\n",
      "iter 548: loss 2.0548, time 40745.66ms, mfu -100.00%\n",
      "iter 549: loss 1.8205, time 40679.53ms, mfu -100.00%\n",
      "step 550: train loss 1.5417, val loss 3.4813\n",
      "saving checkpoint to out\n",
      "iter 550: loss 1.5476, time 193591.74ms, mfu -100.00%\n",
      "iter 551: loss 1.8183, time 40641.05ms, mfu -100.00%\n",
      "iter 552: loss 1.8037, time 40640.40ms, mfu -100.00%\n",
      "iter 553: loss 1.9239, time 40740.46ms, mfu -100.00%\n",
      "iter 554: loss 1.8579, time 40604.43ms, mfu -100.00%\n",
      "iter 555: loss 1.8659, time 40630.10ms, mfu -100.00%\n",
      "iter 556: loss 1.6801, time 40772.76ms, mfu -100.00%\n",
      "iter 557: loss 1.8059, time 40726.31ms, mfu -100.00%\n",
      "iter 558: loss 1.7656, time 40769.45ms, mfu -100.00%\n",
      "iter 559: loss 1.9006, time 40756.90ms, mfu -100.00%\n",
      "iter 560: loss 1.7500, time 40758.25ms, mfu -100.00%\n",
      "iter 561: loss 1.6480, time 40722.66ms, mfu -100.00%\n",
      "iter 562: loss 1.8344, time 40759.69ms, mfu -100.00%\n",
      "iter 563: loss 1.8238, time 40746.18ms, mfu -100.00%\n",
      "iter 564: loss 1.7838, time 40823.13ms, mfu -100.00%\n",
      "iter 565: loss 1.8802, time 40751.10ms, mfu -100.00%\n",
      "iter 566: loss 1.5511, time 40700.20ms, mfu -100.00%\n",
      "iter 567: loss 1.8857, time 40675.94ms, mfu -100.00%\n",
      "iter 568: loss 1.9546, time 40753.89ms, mfu -100.00%\n",
      "iter 569: loss 1.7669, time 40695.37ms, mfu -100.00%\n",
      "iter 570: loss 1.7457, time 40739.30ms, mfu -100.00%\n",
      "iter 571: loss 1.7186, time 40717.21ms, mfu -100.00%\n",
      "iter 572: loss 1.9041, time 40693.89ms, mfu -100.00%\n",
      "iter 573: loss 1.8443, time 40761.19ms, mfu -100.00%\n",
      "iter 574: loss 1.7673, time 40772.95ms, mfu -100.00%\n",
      "iter 575: loss 1.6235, time 40749.34ms, mfu -100.00%\n",
      "iter 576: loss 1.7430, time 40787.77ms, mfu -100.00%\n",
      "iter 577: loss 1.9094, time 40770.73ms, mfu -100.00%\n",
      "iter 578: loss 1.7201, time 40735.92ms, mfu -100.00%\n",
      "iter 579: loss 1.8613, time 40809.95ms, mfu -100.00%\n",
      "iter 580: loss 1.7672, time 40780.90ms, mfu -100.00%\n",
      "iter 581: loss 1.7924, time 40777.55ms, mfu -100.00%\n",
      "iter 582: loss 1.5710, time 40796.41ms, mfu -100.00%\n",
      "iter 583: loss 1.7255, time 40746.09ms, mfu -100.00%\n",
      "iter 584: loss 1.6207, time 40774.25ms, mfu -100.00%\n",
      "iter 585: loss 1.8693, time 40782.22ms, mfu -100.00%\n",
      "iter 586: loss 1.4588, time 40775.28ms, mfu -100.00%\n",
      "iter 587: loss 1.5615, time 40738.63ms, mfu -100.00%\n",
      "iter 588: loss 1.6691, time 40768.09ms, mfu -100.00%\n",
      "iter 589: loss 1.7473, time 40771.45ms, mfu -100.00%\n",
      "iter 590: loss 1.4262, time 40742.92ms, mfu -100.00%\n",
      "iter 591: loss 1.7952, time 40737.35ms, mfu -100.00%\n",
      "iter 592: loss 1.8072, time 40733.53ms, mfu -100.00%\n",
      "iter 593: loss 1.9076, time 40729.48ms, mfu -100.00%\n",
      "iter 594: loss 1.7148, time 40782.84ms, mfu -100.00%\n",
      "iter 595: loss 1.3641, time 40745.74ms, mfu -100.00%\n",
      "iter 596: loss 1.5057, time 40726.71ms, mfu -100.00%\n",
      "iter 597: loss 1.4837, time 40847.01ms, mfu -100.00%\n",
      "iter 598: loss 1.9791, time 40758.90ms, mfu -100.00%\n",
      "iter 599: loss 1.7969, time 40739.72ms, mfu -100.00%\n",
      "step 600: train loss 1.3644, val loss 3.6589\n",
      "saving checkpoint to out\n",
      "iter 600: loss 1.7423, time 193568.97ms, mfu -100.00%\n",
      "iter 601: loss 1.7092, time 40611.07ms, mfu -100.00%\n",
      "iter 602: loss 1.3477, time 40687.65ms, mfu -100.00%\n",
      "iter 603: loss 1.8082, time 40695.28ms, mfu -100.00%\n",
      "iter 604: loss 1.5295, time 40633.05ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 605: loss 1.6134, time 40693.34ms, mfu -100.00%\n",
      "iter 606: loss 1.6291, time 40847.61ms, mfu -100.00%\n",
      "iter 607: loss 1.8166, time 40648.62ms, mfu -100.00%\n",
      "iter 608: loss 1.6683, time 40597.22ms, mfu -100.00%\n",
      "iter 609: loss 1.9093, time 40608.99ms, mfu -100.00%\n",
      "iter 610: loss 1.7614, time 40664.47ms, mfu -100.00%\n",
      "iter 611: loss 1.6427, time 40643.25ms, mfu -100.00%\n",
      "iter 612: loss 1.6602, time 40667.60ms, mfu -100.00%\n",
      "iter 613: loss 1.6709, time 40666.00ms, mfu -100.00%\n",
      "iter 614: loss 1.1384, time 40656.00ms, mfu -100.00%\n",
      "iter 615: loss 1.6192, time 40703.49ms, mfu -100.00%\n",
      "iter 616: loss 1.6002, time 40701.05ms, mfu -100.00%\n",
      "iter 617: loss 1.7900, time 40687.95ms, mfu -100.00%\n",
      "iter 618: loss 1.5131, time 40698.19ms, mfu -100.00%\n",
      "iter 619: loss 1.5105, time 40808.46ms, mfu -100.00%\n",
      "iter 620: loss 1.9595, time 40870.28ms, mfu -100.00%\n",
      "iter 621: loss 1.5282, time 40780.80ms, mfu -100.00%\n",
      "iter 622: loss 1.5858, time 40845.50ms, mfu -100.00%\n",
      "iter 623: loss 1.0325, time 40769.98ms, mfu -100.00%\n",
      "iter 624: loss 1.4807, time 40763.08ms, mfu -100.00%\n",
      "iter 625: loss 1.5438, time 40754.53ms, mfu -100.00%\n",
      "iter 626: loss 1.6447, time 40818.53ms, mfu -100.00%\n",
      "iter 627: loss 1.4757, time 40731.31ms, mfu -100.00%\n",
      "iter 628: loss 1.6577, time 40777.71ms, mfu -100.00%\n",
      "iter 629: loss 1.6533, time 40784.32ms, mfu -100.00%\n",
      "iter 630: loss 1.5908, time 40737.05ms, mfu -100.00%\n",
      "iter 631: loss 1.4455, time 40725.63ms, mfu -100.00%\n",
      "iter 632: loss 1.7800, time 40731.31ms, mfu -100.00%\n",
      "iter 633: loss 1.1715, time 40802.80ms, mfu -100.00%\n",
      "iter 634: loss 1.5581, time 40733.83ms, mfu -100.00%\n",
      "iter 635: loss 1.8831, time 40705.68ms, mfu -100.00%\n",
      "iter 636: loss 1.3690, time 40709.84ms, mfu -100.00%\n",
      "iter 637: loss 1.2122, time 40774.86ms, mfu -100.00%\n",
      "iter 638: loss 1.3625, time 40714.20ms, mfu -100.00%\n",
      "iter 639: loss 1.5006, time 40707.39ms, mfu -100.00%\n",
      "iter 640: loss 1.5930, time 40748.26ms, mfu -100.00%\n",
      "iter 641: loss 1.5095, time 40704.82ms, mfu -100.00%\n",
      "iter 642: loss 1.4815, time 40684.13ms, mfu -100.00%\n",
      "iter 643: loss 1.4810, time 40698.33ms, mfu -100.00%\n",
      "iter 644: loss 1.5274, time 40798.78ms, mfu -100.00%\n",
      "iter 645: loss 1.7870, time 40634.50ms, mfu -100.00%\n",
      "iter 646: loss 1.2017, time 40728.61ms, mfu -100.00%\n",
      "iter 647: loss 1.2869, time 40734.44ms, mfu -100.00%\n",
      "iter 648: loss 1.4011, time 40656.54ms, mfu -100.00%\n",
      "iter 649: loss 1.5157, time 40700.64ms, mfu -100.00%\n",
      "step 650: train loss 1.1741, val loss 3.7669\n",
      "saving checkpoint to out\n",
      "iter 650: loss 1.4934, time 193791.23ms, mfu -100.00%\n",
      "iter 651: loss 1.1865, time 40585.74ms, mfu -100.00%\n",
      "iter 652: loss 1.6075, time 40674.54ms, mfu -100.00%\n",
      "iter 653: loss 1.3601, time 40679.08ms, mfu -100.00%\n",
      "iter 654: loss 1.4957, time 40640.40ms, mfu -100.00%\n",
      "iter 655: loss 1.3236, time 40645.97ms, mfu -100.00%\n",
      "iter 656: loss 1.7007, time 40621.86ms, mfu -100.00%\n",
      "iter 657: loss 1.3980, time 40677.22ms, mfu -100.00%\n",
      "iter 658: loss 1.3907, time 40632.60ms, mfu -100.00%\n",
      "iter 659: loss 1.4647, time 40705.56ms, mfu -100.00%\n",
      "iter 660: loss 1.3501, time 40749.44ms, mfu -100.00%\n",
      "iter 661: loss 1.4895, time 40689.98ms, mfu -100.00%\n",
      "iter 662: loss 1.6463, time 40677.07ms, mfu -100.00%\n",
      "iter 663: loss 1.3885, time 40668.34ms, mfu -100.00%\n",
      "iter 664: loss 1.3097, time 40730.70ms, mfu -100.00%\n",
      "iter 665: loss 1.2503, time 40695.64ms, mfu -100.00%\n",
      "iter 666: loss 1.4604, time 40747.43ms, mfu -100.00%\n",
      "iter 667: loss 1.4694, time 40698.58ms, mfu -100.00%\n",
      "iter 668: loss 1.3947, time 40724.60ms, mfu -100.00%\n",
      "iter 669: loss 1.5530, time 40726.98ms, mfu -100.00%\n",
      "iter 670: loss 1.4803, time 40723.02ms, mfu -100.00%\n",
      "iter 671: loss 1.3475, time 40693.12ms, mfu -100.00%\n",
      "iter 672: loss 1.5415, time 40770.15ms, mfu -100.00%\n",
      "iter 673: loss 1.2910, time 40751.61ms, mfu -100.00%\n",
      "iter 674: loss 1.3941, time 40726.26ms, mfu -100.00%\n",
      "iter 675: loss 1.4368, time 40701.82ms, mfu -100.00%\n",
      "iter 676: loss 1.2006, time 40761.25ms, mfu -100.00%\n",
      "iter 677: loss 1.6211, time 40728.06ms, mfu -100.00%\n",
      "iter 678: loss 0.9588, time 40731.41ms, mfu -100.00%\n",
      "iter 679: loss 1.3075, time 40736.26ms, mfu -100.00%\n",
      "iter 680: loss 1.3420, time 40686.48ms, mfu -100.00%\n",
      "iter 681: loss 1.4150, time 40644.71ms, mfu -100.00%\n",
      "iter 682: loss 1.4688, time 40655.44ms, mfu -100.00%\n",
      "iter 683: loss 1.2580, time 40701.02ms, mfu -100.00%\n",
      "iter 684: loss 1.3737, time 40713.54ms, mfu -100.00%\n",
      "iter 685: loss 1.3707, time 40781.59ms, mfu -100.00%\n",
      "iter 686: loss 1.3942, time 40705.33ms, mfu -100.00%\n",
      "iter 687: loss 1.1809, time 40707.68ms, mfu -100.00%\n",
      "iter 688: loss 1.3545, time 40688.28ms, mfu -100.00%\n",
      "iter 689: loss 1.2104, time 40725.17ms, mfu -100.00%\n",
      "iter 690: loss 1.1250, time 40689.18ms, mfu -100.00%\n",
      "iter 691: loss 1.4024, time 40694.76ms, mfu -100.00%\n",
      "iter 692: loss 1.2776, time 40737.48ms, mfu -100.00%\n",
      "iter 693: loss 1.1784, time 40707.44ms, mfu -100.00%\n",
      "iter 694: loss 1.3080, time 40738.90ms, mfu -100.00%\n",
      "iter 695: loss 1.4763, time 40683.19ms, mfu -100.00%\n",
      "iter 696: loss 1.3158, time 40685.05ms, mfu -100.00%\n",
      "iter 697: loss 1.5733, time 40681.53ms, mfu -100.00%\n",
      "iter 698: loss 1.6418, time 40733.50ms, mfu -100.00%\n",
      "iter 699: loss 1.3617, time 40741.90ms, mfu -100.00%\n",
      "step 700: train loss 0.9952, val loss 3.9451\n",
      "saving checkpoint to out\n",
      "iter 700: loss 1.4052, time 193715.33ms, mfu -100.00%\n",
      "iter 701: loss 1.1841, time 40497.44ms, mfu -100.00%\n",
      "iter 702: loss 1.2827, time 40550.06ms, mfu -100.00%\n",
      "iter 703: loss 1.4394, time 40646.73ms, mfu -100.00%\n",
      "iter 704: loss 1.2251, time 40678.06ms, mfu -100.00%\n",
      "iter 705: loss 1.4039, time 40622.90ms, mfu -100.00%\n",
      "iter 706: loss 1.2369, time 40650.65ms, mfu -100.00%\n",
      "iter 707: loss 1.1207, time 40636.74ms, mfu -100.00%\n",
      "iter 708: loss 1.3836, time 40562.99ms, mfu -100.00%\n",
      "iter 709: loss 1.4306, time 40601.51ms, mfu -100.00%\n",
      "iter 710: loss 1.2519, time 40592.45ms, mfu -100.00%\n",
      "iter 711: loss 1.0621, time 40649.16ms, mfu -100.00%\n",
      "iter 712: loss 1.1709, time 40777.91ms, mfu -100.00%\n",
      "iter 713: loss 1.3327, time 40787.36ms, mfu -100.00%\n",
      "iter 714: loss 1.2615, time 40785.56ms, mfu -100.00%\n",
      "iter 715: loss 1.0813, time 40745.81ms, mfu -100.00%\n",
      "iter 716: loss 1.2322, time 40733.65ms, mfu -100.00%\n",
      "iter 717: loss 1.4404, time 40727.56ms, mfu -100.00%\n",
      "iter 718: loss 1.2837, time 40695.33ms, mfu -100.00%\n",
      "iter 719: loss 1.4176, time 40670.80ms, mfu -100.00%\n",
      "iter 720: loss 1.2563, time 40716.36ms, mfu -100.00%\n",
      "iter 721: loss 0.9184, time 40706.24ms, mfu -100.00%\n",
      "iter 722: loss 1.3149, time 40696.93ms, mfu -100.00%\n",
      "iter 723: loss 1.2331, time 40739.47ms, mfu -100.00%\n",
      "iter 724: loss 1.3769, time 40690.30ms, mfu -100.00%\n",
      "iter 725: loss 1.3433, time 40750.48ms, mfu -100.00%\n",
      "iter 726: loss 1.2984, time 40675.72ms, mfu -100.00%\n",
      "iter 727: loss 0.9684, time 40761.40ms, mfu -100.00%\n",
      "iter 728: loss 1.4032, time 40673.53ms, mfu -100.00%\n",
      "iter 729: loss 1.2161, time 40745.57ms, mfu -100.00%\n",
      "iter 730: loss 1.3093, time 40731.57ms, mfu -100.00%\n",
      "iter 731: loss 1.3847, time 40722.69ms, mfu -100.00%\n",
      "iter 732: loss 1.1531, time 40686.12ms, mfu -100.00%\n",
      "iter 733: loss 1.2120, time 40737.73ms, mfu -100.00%\n",
      "iter 734: loss 1.1039, time 40760.02ms, mfu -100.00%\n",
      "iter 735: loss 1.2992, time 40624.64ms, mfu -100.00%\n",
      "iter 736: loss 1.3361, time 40696.26ms, mfu -100.00%\n",
      "iter 737: loss 1.3071, time 40711.64ms, mfu -100.00%\n",
      "iter 738: loss 1.3282, time 40608.32ms, mfu -100.00%\n",
      "iter 739: loss 1.3004, time 40731.57ms, mfu -100.00%\n",
      "iter 740: loss 1.0753, time 40761.51ms, mfu -100.00%\n",
      "iter 741: loss 1.2144, time 40582.41ms, mfu -100.00%\n",
      "iter 742: loss 1.1785, time 40776.51ms, mfu -100.00%\n",
      "iter 743: loss 1.0452, time 40690.63ms, mfu -100.00%\n",
      "iter 744: loss 1.3387, time 40628.99ms, mfu -100.00%\n",
      "iter 745: loss 1.3147, time 40668.18ms, mfu -100.00%\n",
      "iter 746: loss 1.2459, time 40647.81ms, mfu -100.00%\n",
      "iter 747: loss 1.4236, time 40640.71ms, mfu -100.00%\n",
      "iter 748: loss 1.4022, time 40723.77ms, mfu -100.00%\n",
      "iter 749: loss 1.1447, time 40689.72ms, mfu -100.00%\n",
      "step 750: train loss 0.8124, val loss 4.1467\n",
      "saving checkpoint to out\n",
      "iter 750: loss 1.1912, time 193619.87ms, mfu -100.00%\n",
      "iter 751: loss 0.9897, time 40599.78ms, mfu -100.00%\n",
      "iter 752: loss 1.2510, time 40574.30ms, mfu -100.00%\n",
      "iter 753: loss 0.9550, time 40622.69ms, mfu -100.00%\n",
      "iter 754: loss 1.1871, time 40697.47ms, mfu -100.00%\n",
      "iter 755: loss 1.1265, time 40614.64ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 756: loss 1.1097, time 40722.64ms, mfu -100.00%\n",
      "iter 757: loss 1.3419, time 40737.43ms, mfu -100.00%\n",
      "iter 758: loss 1.1092, time 40617.72ms, mfu -100.00%\n",
      "iter 759: loss 1.1439, time 40714.46ms, mfu -100.00%\n",
      "iter 760: loss 0.9590, time 40961.19ms, mfu -100.00%\n",
      "iter 761: loss 1.2048, time 40189.04ms, mfu -100.00%\n",
      "iter 762: loss 1.2707, time 40227.38ms, mfu -100.00%\n",
      "iter 763: loss 1.2193, time 40200.49ms, mfu -100.00%\n",
      "iter 764: loss 1.1586, time 40207.99ms, mfu -100.00%\n",
      "iter 765: loss 1.0979, time 40211.75ms, mfu -100.00%\n",
      "iter 766: loss 1.1406, time 40191.49ms, mfu -100.00%\n",
      "iter 767: loss 1.1492, time 40246.38ms, mfu -100.00%\n",
      "iter 768: loss 1.2516, time 40206.85ms, mfu -100.00%\n",
      "iter 769: loss 1.2451, time 40208.13ms, mfu -100.00%\n",
      "iter 770: loss 1.1286, time 40238.91ms, mfu -100.00%\n",
      "iter 771: loss 0.9660, time 40193.66ms, mfu -100.00%\n",
      "iter 772: loss 1.0393, time 40236.99ms, mfu -100.00%\n",
      "iter 773: loss 1.1541, time 40222.38ms, mfu -100.00%\n",
      "iter 774: loss 1.3073, time 40211.04ms, mfu -100.00%\n",
      "iter 775: loss 1.1832, time 40249.03ms, mfu -100.00%\n",
      "iter 776: loss 1.0450, time 40236.54ms, mfu -100.00%\n",
      "iter 777: loss 0.9945, time 40214.68ms, mfu -100.00%\n",
      "iter 778: loss 0.9910, time 40200.53ms, mfu -100.00%\n",
      "iter 779: loss 0.9591, time 40212.23ms, mfu -100.00%\n",
      "iter 780: loss 0.8796, time 40217.58ms, mfu -100.00%\n",
      "iter 781: loss 1.0467, time 40279.64ms, mfu -100.00%\n",
      "iter 782: loss 1.0462, time 40203.10ms, mfu -100.00%\n",
      "iter 783: loss 1.1579, time 40208.96ms, mfu -100.00%\n",
      "iter 784: loss 1.0411, time 40232.71ms, mfu -100.00%\n",
      "iter 785: loss 1.2177, time 40222.30ms, mfu -100.00%\n",
      "iter 786: loss 1.1527, time 40223.58ms, mfu -100.00%\n",
      "iter 787: loss 0.9441, time 40203.88ms, mfu -100.00%\n",
      "iter 788: loss 1.1913, time 40221.00ms, mfu -100.00%\n",
      "iter 789: loss 0.9218, time 40240.21ms, mfu -100.00%\n",
      "iter 790: loss 1.0424, time 40231.37ms, mfu -100.00%\n",
      "iter 791: loss 1.0809, time 40208.50ms, mfu -100.00%\n",
      "iter 792: loss 1.0853, time 40236.62ms, mfu -100.00%\n",
      "iter 793: loss 1.1532, time 40230.63ms, mfu -100.00%\n",
      "iter 794: loss 1.1586, time 40241.19ms, mfu -100.00%\n",
      "iter 795: loss 1.0683, time 40262.86ms, mfu -100.00%\n",
      "iter 796: loss 0.9063, time 40218.74ms, mfu -100.00%\n",
      "iter 797: loss 1.2648, time 40229.99ms, mfu -100.00%\n",
      "iter 798: loss 1.1082, time 40262.69ms, mfu -100.00%\n",
      "iter 799: loss 0.9700, time 40270.60ms, mfu -100.00%\n",
      "step 800: train loss 0.6675, val loss 4.2980\n",
      "saving checkpoint to out\n",
      "iter 800: loss 0.9827, time 191021.10ms, mfu -100.00%\n",
      "iter 801: loss 0.8943, time 40487.28ms, mfu -100.00%\n",
      "iter 802: loss 0.8917, time 40610.89ms, mfu -100.00%\n",
      "iter 803: loss 1.0080, time 40604.76ms, mfu -100.00%\n",
      "iter 804: loss 1.1592, time 40578.13ms, mfu -100.00%\n",
      "iter 805: loss 1.0464, time 40610.87ms, mfu -100.00%\n",
      "iter 806: loss 1.0656, time 40580.83ms, mfu -100.00%\n",
      "iter 807: loss 1.1306, time 40596.68ms, mfu -100.00%\n",
      "iter 808: loss 0.8995, time 40606.66ms, mfu -100.00%\n",
      "iter 809: loss 1.0705, time 40581.49ms, mfu -100.00%\n",
      "iter 810: loss 0.9060, time 40580.59ms, mfu -100.00%\n",
      "iter 811: loss 0.9064, time 40663.37ms, mfu -100.00%\n",
      "iter 812: loss 1.0938, time 40612.80ms, mfu -100.00%\n",
      "iter 813: loss 1.0558, time 40616.04ms, mfu -100.00%\n",
      "iter 814: loss 1.1644, time 40620.43ms, mfu -100.00%\n",
      "iter 815: loss 0.9754, time 40651.13ms, mfu -100.00%\n",
      "iter 816: loss 1.0139, time 40655.96ms, mfu -100.00%\n",
      "iter 817: loss 1.0088, time 40636.40ms, mfu -100.00%\n",
      "iter 818: loss 0.9166, time 40623.25ms, mfu -100.00%\n",
      "iter 819: loss 1.0308, time 40651.34ms, mfu -100.00%\n",
      "iter 820: loss 0.8889, time 40620.29ms, mfu -100.00%\n",
      "iter 821: loss 1.0360, time 40647.70ms, mfu -100.00%\n",
      "iter 822: loss 0.8067, time 40624.50ms, mfu -100.00%\n",
      "iter 823: loss 0.9556, time 40569.37ms, mfu -100.00%\n",
      "iter 824: loss 1.0500, time 40564.61ms, mfu -100.00%\n",
      "iter 825: loss 1.0195, time 40652.95ms, mfu -100.00%\n",
      "iter 826: loss 1.0236, time 40602.97ms, mfu -100.00%\n",
      "iter 827: loss 1.0723, time 40608.05ms, mfu -100.00%\n",
      "iter 828: loss 1.0603, time 40605.14ms, mfu -100.00%\n",
      "iter 829: loss 0.8950, time 40558.61ms, mfu -100.00%\n",
      "iter 830: loss 0.7611, time 40581.81ms, mfu -100.00%\n",
      "iter 831: loss 0.9668, time 40564.60ms, mfu -100.00%\n",
      "iter 832: loss 0.8934, time 40584.83ms, mfu -100.00%\n",
      "iter 833: loss 0.8464, time 40617.32ms, mfu -100.00%\n",
      "iter 834: loss 0.9808, time 40572.01ms, mfu -100.00%\n",
      "iter 835: loss 1.1085, time 40557.00ms, mfu -100.00%\n",
      "iter 836: loss 1.1606, time 40594.90ms, mfu -100.00%\n",
      "iter 837: loss 1.0167, time 40517.80ms, mfu -100.00%\n",
      "iter 838: loss 1.0092, time 40588.46ms, mfu -100.00%\n",
      "iter 839: loss 0.7073, time 40580.05ms, mfu -100.00%\n",
      "iter 840: loss 0.8651, time 40569.59ms, mfu -100.00%\n",
      "iter 841: loss 0.9059, time 40573.98ms, mfu -100.00%\n",
      "iter 842: loss 0.9458, time 40583.02ms, mfu -100.00%\n",
      "iter 843: loss 0.9504, time 40582.11ms, mfu -100.00%\n",
      "iter 844: loss 0.7740, time 40613.48ms, mfu -100.00%\n",
      "iter 845: loss 0.8283, time 40586.88ms, mfu -100.00%\n",
      "iter 846: loss 0.8835, time 40558.71ms, mfu -100.00%\n",
      "iter 847: loss 0.9319, time 40639.31ms, mfu -100.00%\n",
      "iter 848: loss 0.9038, time 40592.04ms, mfu -100.00%\n",
      "iter 849: loss 0.7238, time 40537.24ms, mfu -100.00%\n",
      "step 850: train loss 0.5618, val loss 4.4204\n",
      "saving checkpoint to out\n",
      "iter 850: loss 0.9884, time 193036.56ms, mfu -100.00%\n",
      "iter 851: loss 0.7977, time 40525.61ms, mfu -100.00%\n",
      "iter 852: loss 1.1278, time 40509.74ms, mfu -100.00%\n",
      "iter 853: loss 0.8603, time 40498.93ms, mfu -100.00%\n",
      "iter 854: loss 0.8548, time 40533.41ms, mfu -100.00%\n",
      "iter 855: loss 1.0008, time 40619.79ms, mfu -100.00%\n",
      "iter 856: loss 0.8316, time 40572.24ms, mfu -100.00%\n",
      "iter 857: loss 0.8303, time 40518.21ms, mfu -100.00%\n",
      "iter 858: loss 0.9586, time 40541.37ms, mfu -100.00%\n",
      "iter 859: loss 0.8492, time 40574.82ms, mfu -100.00%\n",
      "iter 860: loss 0.7528, time 40600.10ms, mfu -100.00%\n",
      "iter 861: loss 0.9214, time 40556.82ms, mfu -100.00%\n",
      "iter 862: loss 0.7899, time 40564.10ms, mfu -100.00%\n",
      "iter 863: loss 0.9005, time 40521.65ms, mfu -100.00%\n",
      "iter 864: loss 0.8857, time 40604.14ms, mfu -100.00%\n",
      "iter 865: loss 0.9357, time 40623.22ms, mfu -100.00%\n",
      "iter 866: loss 0.7462, time 40593.62ms, mfu -100.00%\n",
      "iter 867: loss 0.8550, time 40628.96ms, mfu -100.00%\n",
      "iter 868: loss 0.7384, time 40634.91ms, mfu -100.00%\n",
      "iter 869: loss 0.7684, time 40573.13ms, mfu -100.00%\n",
      "iter 870: loss 0.6794, time 40622.44ms, mfu -100.00%\n",
      "iter 871: loss 0.8109, time 40579.26ms, mfu -100.00%\n",
      "iter 872: loss 0.8077, time 40590.80ms, mfu -100.00%\n",
      "iter 873: loss 0.9017, time 40571.17ms, mfu -100.00%\n",
      "iter 874: loss 0.9038, time 40639.00ms, mfu -100.00%\n",
      "iter 875: loss 0.8480, time 40589.32ms, mfu -100.00%\n",
      "iter 876: loss 1.0612, time 40598.18ms, mfu -100.00%\n",
      "iter 877: loss 0.6837, time 40650.91ms, mfu -100.00%\n",
      "iter 878: loss 0.7190, time 40592.42ms, mfu -100.00%\n",
      "iter 879: loss 0.8787, time 40597.99ms, mfu -100.00%\n",
      "iter 880: loss 0.9138, time 40557.18ms, mfu -100.00%\n",
      "iter 881: loss 0.7728, time 40622.97ms, mfu -100.00%\n",
      "iter 882: loss 0.7779, time 40597.13ms, mfu -100.00%\n",
      "iter 883: loss 0.8025, time 40549.97ms, mfu -100.00%\n",
      "iter 884: loss 0.7206, time 40596.69ms, mfu -100.00%\n",
      "iter 885: loss 1.0381, time 40622.50ms, mfu -100.00%\n",
      "iter 886: loss 1.0181, time 40555.26ms, mfu -100.00%\n",
      "iter 887: loss 0.7701, time 40574.91ms, mfu -100.00%\n",
      "iter 888: loss 0.7392, time 40614.14ms, mfu -100.00%\n",
      "iter 889: loss 0.8404, time 40571.81ms, mfu -100.00%\n",
      "iter 890: loss 0.8168, time 40588.64ms, mfu -100.00%\n",
      "iter 891: loss 0.7480, time 40616.10ms, mfu -100.00%\n",
      "iter 892: loss 0.7136, time 40631.68ms, mfu -100.00%\n",
      "iter 893: loss 0.8846, time 40614.53ms, mfu -100.00%\n",
      "iter 894: loss 0.8541, time 40561.35ms, mfu -100.00%\n",
      "iter 895: loss 0.8059, time 40620.29ms, mfu -100.00%\n",
      "iter 896: loss 0.8547, time 40564.56ms, mfu -100.00%\n",
      "iter 897: loss 0.6782, time 40631.41ms, mfu -100.00%\n",
      "iter 898: loss 0.6746, time 40592.57ms, mfu -100.00%\n",
      "iter 899: loss 0.7204, time 40600.69ms, mfu -100.00%\n",
      "step 900: train loss 0.4416, val loss 4.6008\n",
      "saving checkpoint to out\n",
      "iter 900: loss 0.8080, time 193096.56ms, mfu -100.00%\n",
      "iter 901: loss 0.8957, time 40486.64ms, mfu -100.00%\n",
      "iter 902: loss 0.7382, time 40501.96ms, mfu -100.00%\n",
      "iter 903: loss 0.6890, time 40509.08ms, mfu -100.00%\n",
      "iter 904: loss 0.8433, time 40461.63ms, mfu -100.00%\n",
      "iter 905: loss 0.6874, time 40492.14ms, mfu -100.00%\n",
      "iter 906: loss 0.8345, time 40524.60ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 907: loss 0.7447, time 40587.83ms, mfu -100.00%\n",
      "iter 908: loss 0.6913, time 40513.85ms, mfu -100.00%\n",
      "iter 909: loss 0.7632, time 40524.33ms, mfu -100.00%\n",
      "iter 910: loss 0.7206, time 40554.26ms, mfu -100.00%\n",
      "iter 911: loss 0.6564, time 40538.95ms, mfu -100.00%\n",
      "iter 912: loss 0.6593, time 40581.05ms, mfu -100.00%\n",
      "iter 913: loss 0.8075, time 40585.99ms, mfu -100.00%\n",
      "iter 914: loss 0.7789, time 40553.48ms, mfu -100.00%\n",
      "iter 915: loss 0.7014, time 40529.48ms, mfu -100.00%\n",
      "iter 916: loss 0.7738, time 40539.69ms, mfu -100.00%\n",
      "iter 917: loss 0.7005, time 40542.01ms, mfu -100.00%\n",
      "iter 918: loss 0.8415, time 40602.40ms, mfu -100.00%\n",
      "iter 919: loss 0.7466, time 40575.99ms, mfu -100.00%\n",
      "iter 920: loss 0.6994, time 40566.79ms, mfu -100.00%\n",
      "iter 921: loss 0.7404, time 40538.26ms, mfu -100.00%\n",
      "iter 922: loss 0.7251, time 40550.86ms, mfu -100.00%\n",
      "iter 923: loss 0.8241, time 40526.06ms, mfu -100.00%\n",
      "iter 924: loss 0.8315, time 40523.99ms, mfu -100.00%\n",
      "iter 925: loss 0.9703, time 40641.30ms, mfu -100.00%\n",
      "iter 926: loss 0.6855, time 40557.22ms, mfu -100.00%\n",
      "iter 927: loss 0.7500, time 40583.33ms, mfu -100.00%\n",
      "iter 928: loss 0.7752, time 40556.65ms, mfu -100.00%\n",
      "iter 929: loss 0.8084, time 40592.30ms, mfu -100.00%\n",
      "iter 930: loss 0.7507, time 40599.11ms, mfu -100.00%\n",
      "iter 931: loss 0.7866, time 40589.97ms, mfu -100.00%\n",
      "iter 932: loss 0.6033, time 40583.08ms, mfu -100.00%\n",
      "iter 933: loss 0.8391, time 40539.14ms, mfu -100.00%\n",
      "iter 934: loss 0.7087, time 40571.17ms, mfu -100.00%\n",
      "iter 935: loss 0.6584, time 40547.89ms, mfu -100.00%\n",
      "iter 936: loss 0.7699, time 40581.09ms, mfu -100.00%\n",
      "iter 937: loss 0.6782, time 40558.23ms, mfu -100.00%\n",
      "iter 938: loss 0.5103, time 40538.43ms, mfu -100.00%\n",
      "iter 939: loss 0.6807, time 40604.98ms, mfu -100.00%\n",
      "iter 940: loss 0.8893, time 40616.90ms, mfu -100.00%\n",
      "iter 941: loss 0.5295, time 40589.06ms, mfu -100.00%\n",
      "iter 942: loss 0.5751, time 40632.42ms, mfu -100.00%\n",
      "iter 943: loss 0.8072, time 40533.02ms, mfu -100.00%\n",
      "iter 944: loss 0.7092, time 40591.13ms, mfu -100.00%\n",
      "iter 945: loss 0.5594, time 40576.02ms, mfu -100.00%\n",
      "iter 946: loss 0.7338, time 40571.30ms, mfu -100.00%\n",
      "iter 947: loss 0.7507, time 40610.25ms, mfu -100.00%\n",
      "iter 948: loss 0.7416, time 40462.41ms, mfu -100.00%\n",
      "iter 949: loss 0.9081, time 40549.23ms, mfu -100.00%\n",
      "step 950: train loss 0.3624, val loss 4.7501\n",
      "saving checkpoint to out\n",
      "iter 950: loss 0.5382, time 193551.18ms, mfu -100.00%\n",
      "iter 951: loss 0.5993, time 40520.41ms, mfu -100.00%\n",
      "iter 952: loss 0.6881, time 40532.75ms, mfu -100.00%\n",
      "iter 953: loss 0.5258, time 40509.73ms, mfu -100.00%\n",
      "iter 954: loss 0.7537, time 40545.18ms, mfu -100.00%\n",
      "iter 955: loss 0.6355, time 40543.56ms, mfu -100.00%\n",
      "iter 956: loss 0.6531, time 40558.12ms, mfu -100.00%\n",
      "iter 957: loss 0.6342, time 40568.97ms, mfu -100.00%\n",
      "iter 958: loss 0.5844, time 40507.76ms, mfu -100.00%\n",
      "iter 959: loss 0.5298, time 40543.20ms, mfu -100.00%\n",
      "iter 960: loss 0.7116, time 40546.53ms, mfu -100.00%\n",
      "iter 961: loss 0.6670, time 40561.80ms, mfu -100.00%\n",
      "iter 962: loss 0.6704, time 40581.34ms, mfu -100.00%\n",
      "iter 963: loss 0.6651, time 40599.64ms, mfu -100.00%\n",
      "iter 964: loss 0.8060, time 40540.62ms, mfu -100.00%\n",
      "iter 965: loss 0.5064, time 40511.37ms, mfu -100.00%\n",
      "iter 966: loss 0.7039, time 40589.88ms, mfu -100.00%\n",
      "iter 967: loss 0.6614, time 40583.60ms, mfu -100.00%\n",
      "iter 968: loss 0.6105, time 40567.36ms, mfu -100.00%\n",
      "iter 969: loss 0.6686, time 40554.00ms, mfu -100.00%\n",
      "iter 970: loss 0.5955, time 40586.45ms, mfu -100.00%\n",
      "iter 971: loss 0.5341, time 40639.49ms, mfu -100.00%\n",
      "iter 972: loss 0.6332, time 40578.03ms, mfu -100.00%\n",
      "iter 973: loss 0.5180, time 40576.33ms, mfu -100.00%\n",
      "iter 974: loss 0.5798, time 40610.18ms, mfu -100.00%\n",
      "iter 975: loss 0.5888, time 40617.30ms, mfu -100.00%\n",
      "iter 976: loss 0.5744, time 40566.60ms, mfu -100.00%\n",
      "iter 977: loss 0.6421, time 40536.94ms, mfu -100.00%\n",
      "iter 978: loss 0.6127, time 40635.03ms, mfu -100.00%\n",
      "iter 979: loss 0.5447, time 40533.86ms, mfu -100.00%\n",
      "iter 980: loss 0.5442, time 40569.88ms, mfu -100.00%\n",
      "iter 981: loss 0.5450, time 40614.66ms, mfu -100.00%\n",
      "iter 982: loss 0.6137, time 40589.17ms, mfu -100.00%\n",
      "iter 983: loss 0.6708, time 40593.73ms, mfu -100.00%\n",
      "iter 984: loss 0.5734, time 40540.65ms, mfu -100.00%\n",
      "iter 985: loss 0.5839, time 40530.10ms, mfu -100.00%\n",
      "iter 986: loss 0.5165, time 40521.27ms, mfu -100.00%\n",
      "iter 987: loss 0.4760, time 40568.86ms, mfu -100.00%\n",
      "iter 988: loss 0.6281, time 40570.73ms, mfu -100.00%\n",
      "iter 989: loss 0.6249, time 40549.06ms, mfu -100.00%\n",
      "iter 990: loss 0.6694, time 40612.59ms, mfu -100.00%\n",
      "iter 991: loss 0.6230, time 40517.46ms, mfu -100.00%\n",
      "iter 992: loss 0.6659, time 40604.90ms, mfu -100.00%\n",
      "iter 993: loss 0.5387, time 40548.33ms, mfu -100.00%\n",
      "iter 994: loss 0.7186, time 40529.49ms, mfu -100.00%\n",
      "iter 995: loss 0.6594, time 40480.75ms, mfu -100.00%\n",
      "iter 996: loss 0.6669, time 40548.05ms, mfu -100.00%\n",
      "iter 997: loss 0.4754, time 40554.12ms, mfu -100.00%\n",
      "iter 998: loss 0.5476, time 40601.07ms, mfu -100.00%\n",
      "iter 999: loss 0.4615, time 40577.26ms, mfu -100.00%\n",
      "step 1000: train loss 0.2849, val loss 4.9256\n",
      "saving checkpoint to out\n",
      "iter 1000: loss 0.4687, time 193418.51ms, mfu -100.00%\n",
      "iter 1001: loss 0.5211, time 40512.96ms, mfu -100.00%\n",
      "iter 1002: loss 0.5623, time 40520.90ms, mfu -100.00%\n",
      "iter 1003: loss 0.7044, time 40501.13ms, mfu -100.00%\n",
      "iter 1004: loss 0.5165, time 40502.73ms, mfu -100.00%\n",
      "iter 1005: loss 0.6298, time 40506.02ms, mfu -100.00%\n",
      "iter 1006: loss 0.7134, time 40551.67ms, mfu -100.00%\n",
      "iter 1007: loss 0.5328, time 40501.23ms, mfu -100.00%\n",
      "iter 1008: loss 0.5750, time 40580.64ms, mfu -100.00%\n",
      "iter 1009: loss 0.5502, time 40595.59ms, mfu -100.00%\n",
      "iter 1010: loss 0.5335, time 40547.41ms, mfu -100.00%\n",
      "iter 1011: loss 0.6165, time 40515.85ms, mfu -100.00%\n",
      "iter 1012: loss 0.5801, time 40561.21ms, mfu -100.00%\n",
      "iter 1013: loss 0.5112, time 40561.30ms, mfu -100.00%\n",
      "iter 1014: loss 0.5680, time 40559.64ms, mfu -100.00%\n",
      "iter 1015: loss 0.6464, time 40623.01ms, mfu -100.00%\n",
      "iter 1016: loss 0.6011, time 40632.89ms, mfu -100.00%\n",
      "iter 1017: loss 0.5431, time 40487.29ms, mfu -100.00%\n",
      "iter 1018: loss 0.4666, time 40604.41ms, mfu -100.00%\n",
      "iter 1019: loss 0.4259, time 40643.03ms, mfu -100.00%\n",
      "iter 1020: loss 0.6367, time 40629.61ms, mfu -100.00%\n",
      "iter 1021: loss 0.5672, time 40637.45ms, mfu -100.00%\n",
      "iter 1022: loss 0.5209, time 40568.63ms, mfu -100.00%\n",
      "iter 1023: loss 0.4380, time 40594.99ms, mfu -100.00%\n",
      "iter 1024: loss 0.5717, time 40564.67ms, mfu -100.00%\n",
      "iter 1025: loss 0.5512, time 40569.74ms, mfu -100.00%\n",
      "iter 1026: loss 0.5485, time 40591.81ms, mfu -100.00%\n",
      "iter 1027: loss 0.5398, time 40543.74ms, mfu -100.00%\n",
      "iter 1028: loss 0.4281, time 40558.83ms, mfu -100.00%\n",
      "iter 1029: loss 0.4853, time 40516.09ms, mfu -100.00%\n",
      "iter 1030: loss 0.6111, time 40560.99ms, mfu -100.00%\n",
      "iter 1031: loss 0.4880, time 40601.89ms, mfu -100.00%\n",
      "iter 1032: loss 0.5853, time 40556.41ms, mfu -100.00%\n",
      "iter 1033: loss 0.5178, time 40576.07ms, mfu -100.00%\n",
      "iter 1034: loss 0.5950, time 40618.66ms, mfu -100.00%\n",
      "iter 1035: loss 0.5468, time 40575.81ms, mfu -100.00%\n",
      "iter 1036: loss 0.5183, time 40517.55ms, mfu -100.00%\n",
      "iter 1037: loss 0.5147, time 40583.48ms, mfu -100.00%\n",
      "iter 1038: loss 0.5262, time 40533.12ms, mfu -100.00%\n",
      "iter 1039: loss 0.5408, time 40613.78ms, mfu -100.00%\n",
      "iter 1040: loss 0.4683, time 40573.22ms, mfu -100.00%\n",
      "iter 1041: loss 0.4451, time 40578.04ms, mfu -100.00%\n",
      "iter 1042: loss 0.5091, time 40584.41ms, mfu -100.00%\n",
      "iter 1043: loss 0.4712, time 40575.67ms, mfu -100.00%\n",
      "iter 1044: loss 0.4948, time 40611.86ms, mfu -100.00%\n",
      "iter 1045: loss 0.5483, time 40680.56ms, mfu -100.00%\n",
      "iter 1046: loss 0.5069, time 40630.00ms, mfu -100.00%\n",
      "iter 1047: loss 0.5490, time 40622.39ms, mfu -100.00%\n",
      "iter 1048: loss 0.4126, time 40650.52ms, mfu -100.00%\n",
      "iter 1049: loss 0.4618, time 40616.01ms, mfu -100.00%\n",
      "step 1050: train loss 0.2435, val loss 5.0038\n",
      "saving checkpoint to out\n",
      "iter 1050: loss 0.5833, time 193727.49ms, mfu -100.00%\n",
      "iter 1051: loss 0.4391, time 40511.06ms, mfu -100.00%\n",
      "iter 1052: loss 0.5849, time 40508.33ms, mfu -100.00%\n",
      "iter 1053: loss 0.6477, time 40568.85ms, mfu -100.00%\n",
      "iter 1054: loss 0.5551, time 40554.46ms, mfu -100.00%\n",
      "iter 1055: loss 0.5881, time 40500.43ms, mfu -100.00%\n",
      "iter 1056: loss 0.4820, time 40567.82ms, mfu -100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1057: loss 0.4750, time 40572.27ms, mfu -100.00%\n",
      "iter 1058: loss 0.6179, time 40566.03ms, mfu -100.00%\n",
      "iter 1059: loss 0.4672, time 40617.79ms, mfu -100.00%\n",
      "iter 1060: loss 0.5321, time 40550.26ms, mfu -100.00%\n",
      "iter 1061: loss 0.4838, time 40548.61ms, mfu -100.00%\n",
      "iter 1062: loss 0.3737, time 40571.90ms, mfu -100.00%\n",
      "iter 1063: loss 0.7142, time 40578.33ms, mfu -100.00%\n",
      "iter 1064: loss 0.4783, time 40546.52ms, mfu -100.00%\n",
      "iter 1065: loss 0.4598, time 40554.89ms, mfu -100.00%\n",
      "iter 1066: loss 0.4807, time 40590.07ms, mfu -100.00%\n",
      "iter 1067: loss 0.3923, time 40487.56ms, mfu -100.00%\n",
      "iter 1068: loss 0.4836, time 40557.87ms, mfu -100.00%\n",
      "iter 1069: loss 0.5034, time 40556.86ms, mfu -100.00%\n",
      "iter 1070: loss 0.3971, time 40531.73ms, mfu -100.00%\n",
      "iter 1071: loss 0.3868, time 40551.91ms, mfu -100.00%\n",
      "iter 1072: loss 0.4608, time 40570.76ms, mfu -100.00%\n",
      "iter 1073: loss 0.4634, time 40535.76ms, mfu -100.00%\n",
      "iter 1074: loss 0.5456, time 40586.56ms, mfu -100.00%\n",
      "iter 1075: loss 0.4890, time 40581.70ms, mfu -100.00%\n",
      "iter 1076: loss 0.5244, time 40552.86ms, mfu -100.00%\n",
      "iter 1077: loss 0.4875, time 40566.38ms, mfu -100.00%\n",
      "iter 1078: loss 0.4454, time 40489.09ms, mfu -100.00%\n",
      "iter 1079: loss 0.5551, time 40538.30ms, mfu -100.00%\n",
      "iter 1080: loss 0.4888, time 40540.99ms, mfu -100.00%\n",
      "iter 1081: loss 0.4041, time 40515.46ms, mfu -100.00%\n",
      "iter 1082: loss 0.4041, time 40563.04ms, mfu -100.00%\n",
      "iter 1083: loss 0.3850, time 40590.72ms, mfu -100.00%\n",
      "iter 1084: loss 0.4199, time 40569.51ms, mfu -100.00%\n",
      "iter 1085: loss 0.3935, time 40603.19ms, mfu -100.00%\n",
      "iter 1086: loss 0.4805, time 40582.58ms, mfu -100.00%\n",
      "iter 1087: loss 0.4966, time 40583.91ms, mfu -100.00%\n",
      "iter 1088: loss 0.4182, time 40606.14ms, mfu -100.00%\n",
      "iter 1089: loss 0.4419, time 40631.00ms, mfu -100.00%\n",
      "iter 1090: loss 0.4008, time 40573.86ms, mfu -100.00%\n",
      "iter 1091: loss 0.4826, time 40653.64ms, mfu -100.00%\n",
      "iter 1092: loss 0.5098, time 40588.63ms, mfu -100.00%\n",
      "iter 1093: loss 0.4222, time 40546.40ms, mfu -100.00%\n",
      "iter 1094: loss 0.4699, time 40546.46ms, mfu -100.00%\n",
      "iter 1095: loss 0.3991, time 40576.99ms, mfu -100.00%\n",
      "iter 1096: loss 0.4156, time 40587.10ms, mfu -100.00%\n",
      "iter 1097: loss 0.4683, time 40557.03ms, mfu -100.00%\n",
      "iter 1098: loss 0.3499, time 40587.81ms, mfu -100.00%\n",
      "iter 1099: loss 0.4443, time 40587.47ms, mfu -100.00%\n",
      "step 1100: train loss 0.2018, val loss 5.2499\n",
      "saving checkpoint to out\n",
      "iter 1100: loss 0.3666, time 193275.20ms, mfu -100.00%\n",
      "iter 1101: loss 0.4557, time 40444.95ms, mfu -100.00%\n",
      "iter 1102: loss 0.4788, time 40512.76ms, mfu -100.00%\n",
      "iter 1103: loss 0.4013, time 40508.96ms, mfu -100.00%\n",
      "iter 1104: loss 0.3935, time 40533.49ms, mfu -100.00%\n",
      "iter 1105: loss 0.4587, time 40489.97ms, mfu -100.00%\n",
      "iter 1106: loss 0.4566, time 40549.20ms, mfu -100.00%\n",
      "iter 1107: loss 0.4430, time 40518.13ms, mfu -100.00%\n",
      "iter 1108: loss 0.4519, time 40513.00ms, mfu -100.00%\n",
      "iter 1109: loss 0.4989, time 40550.38ms, mfu -100.00%\n",
      "iter 1110: loss 0.4274, time 40575.60ms, mfu -100.00%\n",
      "iter 1111: loss 0.4505, time 40462.00ms, mfu -100.00%\n",
      "iter 1112: loss 0.4033, time 40491.56ms, mfu -100.00%\n",
      "iter 1113: loss 0.3027, time 40506.50ms, mfu -100.00%\n",
      "iter 1114: loss 0.3732, time 40585.42ms, mfu -100.00%\n",
      "iter 1115: loss 0.4374, time 40543.00ms, mfu -100.00%\n",
      "iter 1116: loss 0.3382, time 40555.69ms, mfu -100.00%\n",
      "iter 1117: loss 0.3920, time 40651.51ms, mfu -100.00%\n",
      "iter 1118: loss 0.4125, time 40571.92ms, mfu -100.00%\n",
      "iter 1119: loss 0.4681, time 40522.37ms, mfu -100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# backward pass, with gradient scaling if training in fp16\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# clip the gradient\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_clip \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "import time\n",
    "\n",
    "\n",
    "X, Y = get_batch(\"train\")  # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0  # number of iterations in the lifetime of this process\n",
    "raw_model = model\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss(model)\n",
    "        print(\n",
    "            f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "        )\n",
    "        if wandb_log:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"iter\": iter_num,\n",
    "                    \"train/loss\": losses[\"train\"],\n",
    "                    \"val/loss\": losses[\"val\"],\n",
    "                    \"lr\": lr,\n",
    "                    \"mfu\": running_mfu * 100,  # convert to percentage\n",
    "                }\n",
    "            )\n",
    "        if losses[\"val\"] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses[\"val\"]\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    \"model\": raw_model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"model_args\": model_args,\n",
    "                    \"iter_num\": iter_num,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"config\": config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, \"ckpt.pt\"))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        # if ddp:\n",
    "        #     # in DDP training we only need to sync gradients at the last micro step.\n",
    "        #     # the official way to do this is with model.no_sync() context manager, but\n",
    "        #     # I really dislike that this bloats the code and forces us to repeat code\n",
    "        #     # looking at the source of that context manager, it just toggles this variable\n",
    "        #     model.require_backward_grad_sync = (\n",
    "        #         micro_step == gradient_accumulation_steps - 1\n",
    "        #     )\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = (\n",
    "                loss / gradient_accumulation_steps\n",
    "            )  # scale the loss to account for gradient accumulation\n",
    "        \n",
    "#         print(f'loss in micro step: {micro_step} out of {gradient_accumulation_steps} steps is: {loss}')\n",
    "\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(\"train\")\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        # if local_iter_num >= 5:  # let the training loop settle a bit\n",
    "        #     mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "        #     running_mfu = mfu if running_mfu == -1.0 else 0.9 * running_mfu + 0.1 * mfu\n",
    "        print(\n",
    "            f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\"\n",
    "        )\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "start = \"A Derivative is a contract\"\n",
    "start = \"Non-banks are\"\n",
    "start = \"Risk is defined\"\n",
    "num_samples = 3 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 123.65M\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.36 GiB already allocated; 0 bytes free; 3.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m:\n\u001b[0;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(model) \u001b[38;5;66;03m# requires PyTorch 2.0 (optional)\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda\\envs\\GptLab\\lib\\site-packages\\torch\\nn\\modules\\module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 4.00 GiB total capacity; 3.36 GiB already allocated; 0 bytes free; 3.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "model = GPT(gptconf)\n",
    "state_dict = checkpoint['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(globals().items()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
